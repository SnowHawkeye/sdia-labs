{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project\n",
    "## Article: *Bridging the gap between regret minimization and best arm identification, with application to A/B tests*\n",
    "### Students: Hadrien & Emilie SALEM\n",
    "\n",
    "In this notebook, we will attempt to implement some of the algorithms presented in the article, and reproduce some of the experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework and setup\n",
    "\n",
    "In this part we import the relevant functions, and present the classes we developed for the bandit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from framework import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "#### Pulling arms\n",
    "\n",
    "The `Arm` class is just a convenience to draw samples from a certain distribution. Its usage is shown thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0\n",
    "std = 1\n",
    "\n",
    "test_arm = Arm(mean, std, gaussian_sampling)\n",
    "results = test_arm.pull(times=10000)\n",
    "print(f\"Empirical mean = {results.mean()} (theoretical = {mean})\")    \n",
    "print(f\"Empirical std = {results.std()} (theoretical = {std})\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an environment\n",
    "\n",
    "An `Environment` is an object defined by a list of arms. It lets a user pull an arm and exposes the history of observed rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environement\n",
    "test_arm_bis = Arm(mean, std, gaussian_sampling)\n",
    "test_env = Environment([test_arm, test_arm_bis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull an arm\n",
    "test_env.pull_arm(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the reward history\n",
    "test_env.reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the reward history\n",
    "test_env.reset_history()\n",
    "test_env.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with an Explore-Then-Commit (ETC) agent\n",
    "\n",
    "We implement an ETC agent such as described in the article (*cf*. page 2). We then compare the theoretical regret to the empirical regret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(agent, confidence, std, mean1, mean2, n_steps, n_experiments):\n",
    "    # Create an environment with 2 arms\n",
    "    arm1 = Arm(mean1, std, gaussian_sampling)\n",
    "    arm2 = Arm(mean2, std, gaussian_sampling)\n",
    "    env = Environment([arm1, arm2])\n",
    "    \n",
    "    regrets = []\n",
    "    decision_times = []\n",
    "\n",
    "    for _  in range(n_experiments):\n",
    "        results = agent.play(n_steps, confidence, env)\n",
    "        regrets.append(results.decision_regret)\n",
    "        decision_times.append(results.decision_time)\n",
    "        \n",
    "    return np.mean(regrets), np.mean(decision_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the regret for a large number of experiments in order to increase the precision of empirical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ETC_Agent()\n",
    "confidence = 0.02\n",
    "std = 1\n",
    "mean1 = 0\n",
    "mean2 = 1\n",
    "n_steps = 100\n",
    "n_experiments = 1000\n",
    "\n",
    "regret, decision_time = run_experiment(agent, confidence, std, mean1, mean2, n_steps, n_experiments)\n",
    "\n",
    "print(f\"Average regret at time of decision: {np.mean(regret)}, Average decision time: {np.mean(decision_time)}.\")\n",
    "theoretical_regret = (8*std**2)/(mean2-mean1)*np.log(1/confidence)\n",
    "print(f\"Theoretical regret bound: {theoretical_regret}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the article, the regret at the time of decision is bound by a number slightly larger than\n",
    "$\\frac{8\\sigma^2}{\\Delta}\\log(1/\\delta)$\n",
    ", which seems to be respected in the experiment above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The UCB $_{\\alpha}$ algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement and experiment with the $UCB_{\\alpha}$ algorithm proposed in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient needed to compute the theoretical regret\n",
    "def c(alpha):\n",
    "    if alpha == 1 : return 1\n",
    "    else : return np.min([(alpha+1)**2 / 4, 4*alpha**2 / (1 - alpha)**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "agent = UCBAlpha_Agent(alpha)\n",
    "confidence = 0.02\n",
    "std = 1\n",
    "mean1 = 0\n",
    "mean2 = 1\n",
    "n_steps = 1000\n",
    "n_experiments = 100\n",
    "\n",
    "regret, decision_time = run_experiment(agent, confidence, std, mean1, mean2, n_steps, n_experiments)\n",
    "\n",
    "print(f\"Average regret at time of decision: {np.mean(regret)}, Average decision time: {np.mean(decision_time)}.\")\n",
    "delta = abs(mean2 - mean1)\n",
    "theoretical_regret = ( (8*std**2 / delta)*c(alpha) + delta ) * np.log(1/confidence)\n",
    "print(f\"Theoretical regret bound: {theoretical_regret}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation of the $UCB_{\\alpha}$ algorithm seems to have the expected behaviour: a low $\\alpha$ reduces the regret at the cost of a higher decision time, and *vice-versa*.\n",
    "\n",
    "We do note that we capped the number of steps at 1000 in order to have reasonable computing times. This means that although tendencies are as expected, the displayed numbers for decision times have no statistical value when they get close to 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between the algorithms\n",
    "\n",
    "Now that we have functional agents and algorithms, we are able to reproduce the experiments realized in the article for the i.i.d. case with two arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 1\n",
    "mean1 = 0\n",
    "mean2 = 1\n",
    "n_steps = 1000\n",
    "n_experiments = 100\n",
    "log_inv_delta_range = np.linspace(0, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulation with UCB_alpha\n",
    "alphas = [1, 2, 4, 32, 1000]\n",
    "\n",
    "regrets_ucb = []\n",
    "decision_times_ucb = []\n",
    "\n",
    "for alpha in alphas :\n",
    "    print(f\"Simulating alpha = {alpha}...\", end=\" \")\n",
    "    agent_alpha = UCBAlpha_Agent(alpha)\n",
    "    \n",
    "    regrets_alpha = []\n",
    "    decision_times_alpha = []\n",
    "    \n",
    "    for log_inv_delta in log_inv_delta_range :\n",
    "        delta = np.exp(-log_inv_delta)\n",
    "        regret, decision_time = run_experiment(agent_alpha, delta, std, mean1, mean2, n_steps, n_experiments)\n",
    "        \n",
    "        regrets_alpha.append(regret)\n",
    "        decision_times_alpha.append(decision_time)\n",
    "        \n",
    "    regrets_ucb.append(regrets_alpha)\n",
    "    decision_times_ucb.append(decision_times_alpha)\n",
    "    \n",
    "    print(\"done !\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation with ETC     \n",
    "agent_etc = ETC_Agent()\n",
    "\n",
    "regrets_etc = []\n",
    "decision_times_etc = []\n",
    "\n",
    "for log_inv_delta in log_inv_delta_range :\n",
    "    delta = np.exp(-log_inv_delta)\n",
    "    regret, decision_time = run_experiment(agent_etc, delta, std, mean1, mean2, n_steps, n_experiments)\n",
    "    \n",
    "    regrets_etc.append(regret)\n",
    "    decision_times_etc.append(decision_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.xlabel(\"log(1/δ)\")\n",
    "plt.ylabel(\"Regret\")\n",
    "\n",
    "X = log_inv_delta_range\n",
    "for Y in regrets_ucb :\n",
    "    plt.plot(X, Y)\n",
    "plt.plot(X, regrets_etc)\n",
    "\n",
    "ETC_regret_bound = [ (8*std**2)/(mean2-mean1) * l for l in log_inv_delta_range]\n",
    "plt.plot(X, ETC_regret_bound, 'b--')\n",
    "\n",
    "plt.legend([\"alpha = 1\", \"alpha = 2\", \"alpha = 4\", \"alpha = 32\", \"alpha = 1000 (≈ ETC')\", \"ETC\", \"ETC bound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.xlabel(\"log(1/δ)\")\n",
    "plt.ylabel(\"Decision time\")\n",
    "\n",
    "X = log_inv_delta_range\n",
    "for Y in decision_times_ucb :\n",
    "    plt.plot(X, Y)\n",
    "plt.plot(X, decision_times_etc)\n",
    "\n",
    "plt.legend([\"alpha = 1\", \"alpha = 2\", \"alpha = 4\", \"alpha = 32\", \"alpha = 1000 (≈ ETC')\", \"ETC\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34ff3397c474938b265d2f4b45024e5465249fab18e07736c9a068b37b408800"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('python-all': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
