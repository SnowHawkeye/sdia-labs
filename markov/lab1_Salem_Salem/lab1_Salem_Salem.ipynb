{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Probability 2 (Data science Master, University of Lille) / CMF, (Centrale Lille, G3 SDIA)\n",
        "\n",
        "---\n",
        "\n",
        "## Lab 1 - Discrete time homogeneous Markov chains\n",
        "\n",
        "### Elève 1 : Hadrien SALEM\n",
        "### Elève 2 : Emilie SALEM\n",
        "\n",
        "---\n",
        "\n",
        "## Guidelines (read carefully before starting)\n",
        "\n",
        "**Objectives**: numerically simulate basic Markov chains (discrete time and discrete state space).\n",
        "\n",
        "**Setup**: after retrieving the resources for the lab on moodle:\n",
        "- place the .zip archive in a local folder (Computer -> Documents/Python/);\n",
        "- unzip the archive .zip;\n",
        "- rename the folder with the convention lab1_Name1_Name2\n",
        "- duplicate the notebook file and rename it lab1_Name1_Name2.ipynb;\n",
        "- [**optional, possibly needed if working from Centrale's machines**]\n",
        "    - create a `lab1` conda environment from the provided `requirement.txt` file\n",
        "    ```bash\n",
        "    conda create --name=lab1 --file=requirement.txt\n",
        "    conda activate lab1\n",
        "    # do not forget to deactivate the environment if needed\n",
        "    # you can remove the environment once you are done\n",
        "    conda env remove --name=lab1\n",
        "    ```\n",
        "    - launch jupyter notebook (the python environment will be the one from the conda environment `lab1`)\n",
        "- at the end of the session, do not forget to transfer your work to your own network space if you are working on a machine from the school (do not leave your work on the C: drive).\n",
        "\n",
        "**Assessment** &#8594; grade /20 (possibly converted later on to a grade ranging from F to A (A+))\n",
        "\n",
        "This lab session will be evaluated, based on your answer to the exercises reported in a Jupyter notebook (e.g., this one) and any additional `.py` file produced. In particular:\n",
        "\n",
        "- make sure the notebook you produce is properly annotated: each exercise should be introduced by a short sentence summarizing its context. Concisely answer each question from the guideline. \n",
        "- **relate the numerical results to the theory covered during the lecture** whenever appropriate;\n",
        "- **codes without any explanations (in a text cell) will not be considered as a complete answer**, and will only be attributed a fraction of the grade attributed to the question.\n",
        "- any code produced should be commented whenever appropriate;\n",
        "- include appropriate axis labels and a relevant title to each figure reported in the notebook;\n",
        "- **document any function you introduce (using docstrings)**, and comment your code whenever appropriate (see, *e.g.*, [PEP 8 style recommendations](https://www.python.org/dev/peps/pep-0008/)). \n",
        "     - use a reference docstring style, *e.g.*, the [google docstring style](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).\n",
        "- **give a clear structure to your notebook**: longer theoretical explanations should be reported in markdown cells. Include LaTeX equations to improve the clarity of the document.\n",
        "\n",
        "**Additional evaluation criteria**:\n",
        "- Code clarity / conciseness / documentation\n",
        "- Comment quality / overall presentation    \n",
        "\n",
        "## <a name=\"content\">Contents</a>\n",
        "- [Exercise 1: Ehrenfest model](attachment:./#ex1)\n",
        "- [Exercise 2: Simulation of a discrete time homogeneous Markov chain](attachment:./#ex2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## <a name=\"ex1\">Exercise 1: Ehrenfest model</a> [(&#8593;)](attachment:./#content) <!-- [$\\cdot$/10] -->\n",
        " Consider a system of $K = 30$ particles (labeled from 1 to $K$) evolving in a closed box. The box is divided into two compartments in contact with each other, respectively identified by an index, $0$ and $1$. A hole at the interface between the two compartments allows the particles to move from one compartment to the other.\n",
        "\n",
        " Particle motion is modeled as follows: at each discrete time intant $n$, one particle is chosen uniformly at random and moved from its current compartment to the other. Let $X(n)$ denote the number of particles in compartment $0$ at time $n$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1\\. <!--[$\\cdot$/0.5]--> Briefly justify that $\\bigl(X(n) \\bigr)_{n \\in \\mathbb{N}}$ is a Markov chain."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Il s'agit bien d'un **processus à temps discret**, où $\\bigl(X(n) \\bigr)_{n \\in \\mathbb{N}}$ est une suite de variables aléatoires à valeurs dans l'ensemble d'états $S = \\{0, 1, ... 30\\}$ correspondant au nombre de particules pouvant se trouver dans le compartiment $0$ au pas de temps $n$.\n",
        "- A chaque étape, le passage d'une particule d'un compartiment à l'autre de la boîte est indépendant des étapes précédentes, et donc le nombre de particules dans le compartiment $0$ ne dépend que du nombre de particules dans ce compartiment à l'étape précédente (**processus sans mémoire**).\n",
        "\n",
        "Il s'agit donc bien d'une **chaîne de Markov**."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2\\. <!--[$\\cdot$/1.5]--> Is the chain irreducible? (Positive) recurrent? Aperiodic or periodic? Prove each statement from your answer (recall the main steps covered during the exercise session)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On étudie les probabilités de transition :\n",
        "- si $X_0 = 0$, une particule passe nécessairement de $0$ à $1$\n",
        "- si $X_0 = K$, une particule passe nécessairement de $1$ à $0$\n",
        "- si $X_0 \\in \\{1, ..., K-1\\}$\n",
        "    - la probabilité de passage d'une particule de $0$ à $1$ est $\\frac{j}{K}$ par hypothèse d'uniformité\n",
        "    - de même, la probabilité de passage d'une particule de $1$ à $0$ est $1 - \\frac{j}{K}$\n",
        "\n",
        "Ainsi, on a : \n",
        "$$\n",
        "\\begin{cases}\n",
        "\n",
        "\\mathbb{P}(X_1 = 1 | X_0 = 0) = 1 \\\\\n",
        "\\mathbb{P}(X_1 = K-1 | X_0 = K) = 1 \\\\\n",
        "\\mathbb{P}(X_1 = j+1 | X_0 = j) = 1 - \\frac{j}{K} & \\text{si } j \\notin \\{ 0; K \\}\\\\\n",
        "\\mathbb{P}(X_1 = j-1 | X_0 = j) = \\frac{j}{K} & \\text{si } j \\notin \\{ 0; K \\}\\\\\n",
        "\\mathbb{P}(X_1 = j | X_0 = i) = 0 & \\text{si } i \\notin \\{ j-1; j+1 \\} \\\\\n",
        "\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "(Les probabilités de transition sont données pour le passage de $0$ à $1$ mais se généralisent par homogénéité de la chaîne.)\n",
        "\n",
        "Ainsi, il existe un cycle avec probabilités positives joignant l'état $0$ à l'état $K$ passant par tous les états, et revenant en $0$ : la chaîne est **irréductible**.\n",
        "\n",
        "La chaîne est également **récurrente** puisque l'espace d'états est fini et qu'elle n'admet aucun état absorbant.\n",
        "\n",
        "De plus, on passe des sites pairs aux sites impairs en un pas de temps, et vice-versa : **la chaîne est périodique de période 2**."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3\\. <!--[$\\cdot$/0.5]--> Recall the structure of the transition matrix, and encode it in Python (without any for loop).\n",
        "\n",
        " > Hint: use the function `numpy.diag`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "D'après les probabilités établies à la question précédente, la matrice de transition a la forme suivante :\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "0            & 1     & 0      &  \\ldots      & 0     & 0  \\\\\n",
        "\\frac{1}{K}  & 0 & \\frac{K-1}{K} & \\ddots &    & 0 \\\\\n",
        "0            & \\frac{2}{K} & \\ddots & \\ddots & \\ddots & \\vdots   \\\\\n",
        "\\vdots       & \\ddots & \\ddots & \\ddots & \\ddots & 0  \\\\\n",
        "0            &        & \\ddots & \\ddots & \\ddots & \\frac{1}{K} \\\\\n",
        "0            & 0      & \\ldots      & 0     & 1      & 0\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Nombre de particules\n",
        "K = 30\n",
        "\n",
        "# Calcul de la sous-diagonale\n",
        "diag_1 = (1/K)*np.arange(1, K+1)\n",
        "    \n",
        "# Calcul de la sur-diagonale : il s'agit simplement des coefficients de la sous-diagonale, mais pris dans \"l'autre sens\"\n",
        "diag_2 = np.flip(diag_1)\n",
        "\n",
        "# On crée la matrice de transisition en ajoutant la sous-diagonale et la sur-diagonale à une matrice de zéros\n",
        "P = np.zeros(shape=(K+1,K+1)) + np.diag(diag_1, k=-1) + np.diag(diag_2, k=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4\\. <!--[$\\cdot$/0.5]-->Numerically verify that the binomial distribution $\\mathcal{B} (K, 1/2)$ is invariant for the chain. This invariant distribution will be denoted by $\\pi$ in the rest of the exercise.\n",
        "\n",
        " > Hint: you can use `scipy.stats.binom`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous allons vérifier numériquement que le vecteur $\\pi$ correspondant à la distribution $\\mathcal{B} (K, 1/2)$ vérifie : $\\pi P = \\pi$ : cela prouvera alors que la distribution en question est invariante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import binom\n",
        "\n",
        "\n",
        "distrib_bin = binom(K, 0.5)\n",
        "\n",
        "# Crée un array contenant les probabilités de la loi binomiale pour k allant de 0 à K\n",
        "pi = distrib_bin.pmf(np.arange(0,K+1))\n",
        "\n",
        "# Vérification numérique (on laisse une certaine tolérance pour les approximations numériques)\n",
        "print(f\"pi is equal to pi*P >> {np.all(pi.dot(P) - pi < 1e-15)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5\\. <!--[$\\cdot$/2]--> Implement a Python function `ehrenfest` to simulate a trajectory of the chain for a system of $K$ particles, for initial distribution $\\mu$ (for instance a Dirac centered in $0$, meaning that the compartment $0$ is empty at $n = 0$). The maximum number of time steps will be controlled by an input parameter $n_{\\max}$.\n",
        "\n",
        "For an efficient implementation, **do not use vector-matrix product with the transition matrix implemented in 3**: rely on the description of the system instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous définissons une fonction `ehrenfest(K, n_max, rng)` dans un fichier annexe `lab1_functions.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lab1_functions import ehrenfest"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6\\. Simulate a trajectory of the chain starting in state $0$ for $n_{\\max} = 5000$. Display the result in function of the time index $n$. Briefly describe the curve you obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(seed=0)\n",
        "\n",
        "n_max = 5000\n",
        "simulation_states, T = ehrenfest(K, n_max, rng)\n",
        "simulation_distrib = np.bincount(simulation_states.astype(int))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "\n",
        "# Graphe complet\n",
        "plt.subplot(131)\n",
        "plt.title(\"Simulation d'une trajectoire en suivant le modèle d'Ehrenfest\")\n",
        "plt.xlabel(\"Etape\")\n",
        "plt.ylabel(\"Etat\")\n",
        "plt.plot(simulation_states)\n",
        "\n",
        "# Seulement les premiers états pour avoir un graphe lisible\n",
        "plt.subplot(132)\n",
        "plt.title(\"100 premiers états de la simulation\")\n",
        "plt.xlabel(\"Etape\")\n",
        "plt.ylabel(\"Etat\")\n",
        "plt.plot(simulation_states[:100])\n",
        "\n",
        "# Histogramme des états\n",
        "plt.subplot(133)\n",
        "plt.title(\"Distribution des états\")\n",
        "plt.xlabel(\"Etat\")\n",
        "plt.ylabel(\"Nombre de valeurs\")\n",
        "plt.stem(simulation_distrib)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comme on peut s'y attendre au vu de la formulation du problème, les valeurs prises par $X_n$ se concentrent autour des valeurs \"médianes\" (entre 10 et 20). Nous verrons dans les questions suivantes que la distribution se rapproche en fait d'une distribution binomiale.\n",
        "\n",
        "On remarque également que $X_n$ ne prend jamais de valeur supérieure à 25. Cela s'explique par la faible probabilité de ces valeurs dans la distribution limite (cf. question suivante), couplée au fait que l'on commence à l'état 0 (qui en est le plus éloigné)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7\\. Compare the empirical histogram of the trajectory obtained in 5. to the theoretical limit distribution $\\pi$. What do you observe?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On affiche sur la même figure la distribution empirique (normalisée) et la distribution binomiale théorique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,7))\n",
        "plt.title(\"Comparaison entre les distributions théorique et expérimentale\")\n",
        "plt.stem(pi, linefmt=\"red\", markerfmt=\"ro\")\n",
        "plt.xlabel(\"Etat\")\n",
        "plt.ylabel(\"Nombre de valeurs\")\n",
        "plt.stem(simulation_distrib/n_max, linefmt=\"blue\", markerfmt=\"bo\") # On normalise la distribution expérimentale pour pouvoir la comparer à la distribution théorique\n",
        "plt.legend([\"Distribution théorique\", \"Distribution expérimentale\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On observe effectivement que les distributions sont proches. Plus on augmente la valeur de $n_{max}$, plus on fait tendre $n$ vers l'infini et plus la distribution empirique se rapproche de la distribution théorique."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8\\. a) Modify the function defined in 1. so that it returns the return time to state 0, defined as $T_{0,0} = \\inf \\bigl\\{ n > 0, X(n) = 0 \\mid X(0) = 0 \\bigr\\}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous avons modifié la fonction directement dans le fichier `lab1_functions.py` pour qu'elle renvoie un entier correspondant au premier temps de retour en $0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Le temps de premier retour en 0 est de {T} itérations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dans la distribution théorique limite (loi binomiale de paramètres $K$, $\\frac{1}{2}$), la probabilité de l'état $0$ vaut $\\frac{1}{2^{K}} = \\frac{1}{2^{30}}$ qui est de l'ordre de $10^{-10}$. Ce qui explique qu'en pratique, on n'observe pas de retour en $0$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8\\. b) [**Optional**] Run several chains (about 5, ideally in parallel) for $K = 10$, $n_{\\max} = 5000$, and compare the empirical average of $T_{0,0}$ to $\\pi(0)$. What do you observe?\n",
        " > Hint: a good tutorial showing how to run functions in parallel in Python is available [here](https://www.machinelearningplus.com/python/parallel-processing-python/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8\\. c) Comment on the possibility of numerically observing the chain returning to its initial state as $K$ increases."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plus on augmente la valeur de K, plus la probabilité de revenir à l'état 0 est faible. Sachant que numériquement, on a $10^{-16} \\approx 0$ (précision machine), il deviendra presque impossible d'observer empiriquement un temps de retour non nul."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## <a name=\"ex2\">Exercise 2: Simulation of a discrete time homogeneous Markov chain</a> [(&#8593;)](attachment:./#content)\n",
        " Let $\\bigl( X(n) \\bigr)_{n \\in \\mathbb{N}}$ be a discrete time homogeneous Markov chain defined by the following initial distribution $\\mu$ and transition matrix $P$\n",
        "\n",
        "$$\n",
        "     \\mu = [0, 1, 0, 0, 0, 0], \n",
        "     %\n",
        "     \\quad\n",
        "     %\n",
        "     P = \\begin{pmatrix}\n",
        "       1/2   & 1/2 &0  &0   &0   &0   \\\\\n",
        "\t \t1/4 &0   &0  &1/4 &1/4 &1/4   \\\\\n",
        "       1/2   &0   &0  &0   &0   &1/2 \\\\\n",
        "       0   &1/2 &0  &0   &1/2 &0   \\\\\n",
        "       0   &1/3 &0  &1/3 &0   &1/3 \\\\\n",
        "       0   &1/3 &1/3  & 0 &1/3   &0\n",
        "     \\end{pmatrix}.\n",
        " $$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1\\. What can you say about the Markov chain $X$? (irreducibility, positive recurrence, periodicity, ...). Justify each of your claim, citing the relevant results from the lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- On peut facilement montrer que tous les états sont dans la même classe d'équivalence : il existe un chemin de probabilité non-nulle entre chacun d'entre eux. La chaîne de Markov est donc **irréductible**.\n",
        "- La chaîne étant irréductible sur un espace d'états fini, elle est **récurrente positive**.\n",
        "- La probabilité de rester dans l'état 1 quand on s'y trouve déjà est non-nulle : la chaîne est donc **apériodique** puisque l'on ne peut alors pas exhiber de partition de l'ensemble d'états telle que lorsque l'on est dans l'état 1, on est sûr de passer dans un autre élement de la partition."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2\\. Write a function `simulate_dthmc` simulating the trajectory of the Markov chain $\\bigl( X(n) \\bigr)_{n \\in \\mathbb{N}}$ for $n_{\\max}$ time steps. The signature of the function should include the following elements:\n",
        "   - list of inputs: transition matrix $P$, initial distribution $\\mu$, number of time steps $n_{\\max}$;\n",
        "   - output: array of lenght $n_{\\max}$ representing the trajectory.\n",
        "   \n",
        "**To this end, do not use matrix vector products, which would lead to an extremely inefficient algorithm in this case.**\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On programme la fonction dans le fichier annexe `lab1_functions.py`.\n",
        "Pour le passage d'une étape à l'autre, notre implémentation utilise :\n",
        "$$\n",
        "\n",
        "\\mathbb P(X_{k+1} = j | X_k = i) = p_{ij} \n",
        "\n",
        "$$\n",
        "\n",
        "où $p_{ij}$ sont les coefficients de la matrice de transition. Autrement dit, on trouve la distribution de probabilités en lisant la ligne $i$ de $P$.\n",
        "\n",
        "On utilise alors la fonction `numpy.random.choice` pour générer l'état suivant selon cette distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lab1_functions import simulate_dthmc"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3\\. Simulate a trajectory of the chain for $n_{\\max} = 2000$ starting from $X(0) = 1$. Plot the histogram of the states visited by the chain.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Définition des données\n",
        "mu = np.array([0, 1, 0, 0, 0, 0]) # On commence à l'état 1\n",
        "\n",
        "P = np.array([\n",
        "    [.5, .5, 0, 0, 0, 0],\n",
        "    [.25, 0, 0, .25, .25, .25],\n",
        "    [.5, 0, 0, 0, 0, .5],\n",
        "    [0, .5, 0, 0, .5, 0],\n",
        "    [0, 1/3, 0, 1/3, 0, 1/3],\n",
        "    [0, 1/3, 1/3, 0, 1/3, 0],\n",
        "])\n",
        "\n",
        "n_max = 2000\n",
        "\n",
        "rng = np.random.default_rng(seed=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulation\n",
        "X = simulate_dthmc(P, mu, n_max, rng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Affichage de l'histogramme des états\n",
        "X_distrib = np.bincount(X.astype(int)) \n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "plt.title(\"Distribution des états\")\n",
        "plt.xlabel(\"Etat\")\n",
        "plt.ylabel(\"Nombre de valeurs\")\n",
        "plt.stem(X_distrib/ n_max) # Normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On observe que l'état le plus visité est l'état 1. En effet, on aurait pu intuiter ce résultat puisque de nombreux chemins mènent à cet état."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4\\. Determine numerically an invariant distribution $\\boldsymbol{\\pi}$ of the chain (*e.g.*, based on an eigendecomposition [numpy.linalg.eig](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html)). Is it unique? Compare it to the histogram obtained in 2 (superimpose graphs). What can you conclude?\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Une mesure invariante $\\pi$ vérifie :\n",
        "\n",
        "$$\n",
        "\\pi P = \\pi \\iff P^T \\pi = \\pi\n",
        "$$\n",
        "\n",
        "Ainsi, une telle mesure est un vecteur propre de $P^T$ associé à la valeur propre $1$.\n",
        "\n",
        "Nous commençons par récupérer les valeurs propres et les vecteurs propres de $P^T$ :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eigenvalues, eigenvectors = np.linalg.eig(P.T)\n",
        "print(eigenvalues, eigenvectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le vecteur propre associé à la valeur propre 1 est donc donné par `eigenvectors[:, 0]` (la première colonne de la matrice dans la décomposition en éléments propres), dont toutes les composantes sont réelles. De plus, on normalise le vecteur obtenu afin d'avoir une probabilité invariante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pi = eigenvectors[:, 0].astype(float) \n",
        "pi = pi / sum(pi) # Normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vérifions maintenant que l'on a bien la relation $\\pi P = \\pi$ :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"pi is equal to pi*P >> {np.all(P.T.dot(pi) - pi < 1e-15)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On superpose la distribution théorique trouvée à la distribution empirique observée : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,7))\n",
        "plt.title(\"Comparaison entre les distributions théorique et expérimentale\")\n",
        "plt.stem(pi, linefmt=\"red\", markerfmt=\"ro\")\n",
        "plt.stem(X_distrib/n_max, linefmt=\"blue\", markerfmt=\"bo\")\n",
        "plt.legend([\"Distribution théorique\", \"Distribution expérimentale\"])\n",
        "plt.xlabel(\"Etat\")\n",
        "plt.ylabel(\"Nombre de valeurs\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On retrouve le résultat de cours : la chaîne de Markov étant irréductible, récurrente positive et apériodique, elle admet une unique probabilité invariante vers laquelle elle tend. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5\\. a) Compute $\\mu_n \\triangleq \\mu P^n$, the probability distribution of $X(n)$. What is the limit of $\\mu_n$ as $n$ goes to $+\\infty$? Illustrate the result numerically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "D'après le résultat cité ci-dessus, la limite de $\\mu_n$ est la probabilité invariante $\\pi$.\n",
        "\n",
        "Nous calculons donc numériquement les valeurs successives prises par $\\mu_n$ afin de les comparer avec $\\pi$ dans la question 5.b)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N = 20\n",
        "time = np.arange(N)\n",
        "mu_n = np.zeros((N, 6))\n",
        "for n in time :\n",
        "    mu_n[n] = mu.dot(np.linalg.matrix_power(P, n))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5\\. b) Display on the same graph the curves $n \\mapsto \\mu_n(i)$ for $i = 1, \\dotsc , 6$, and compare with $\\pi$. Display on another graph the function $n \\mapsto \\Vert \\mu_n - \\pi \\Vert_1$, where $\\Vert \\cdot \\Vert_1$ is the $\\ell_1$ norm. What does each of these curves illustrate?\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "plt.subplot(231)\n",
        "plt.title(\"P(X_n = 0)\")\n",
        "plt.plot(mu_n[:, 0])\n",
        "plt.plot(np.ones(N)*pi[0])\n",
        "plt.legend([\"mu_n\", \"pi\"])\n",
        "\n",
        "plt.subplot(232)\n",
        "plt.title(\"P(X_n = 1)\")\n",
        "plt.plot(mu_n[:, 1])\n",
        "plt.plot(np.ones(N)*pi[1])\n",
        "plt.legend([\"mu_n\", \"pi\"])\n",
        "\n",
        "plt.subplot(233)\n",
        "plt.title(\"P(X_n = 2)\")\n",
        "plt.plot(mu_n[:, 2])\n",
        "plt.plot(np.ones(N)*pi[2])\n",
        "plt.legend([\"mu_n\", \"pi\"])\n",
        "\n",
        "plt.subplot(234)\n",
        "plt.title(\"P(X_n = 3)\")\n",
        "plt.plot(mu_n[:, 3])\n",
        "plt.plot(np.ones(N)*pi[3])\n",
        "plt.legend([\"mu_n\", \"pi\"])\n",
        "\n",
        "plt.subplot(235)\n",
        "plt.title(\"P(X_n = 4)\")\n",
        "plt.plot(mu_n[:, 4])\n",
        "plt.plot(np.ones(N)*pi[4])\n",
        "plt.legend([\"mu_n\", \"pi\"])\n",
        "\n",
        "plt.subplot(236)\n",
        "plt.title(\"P(X_n = 5)\")\n",
        "plt.plot(mu_n[:, 5])\n",
        "plt.plot(np.ones(N)*pi[5])\n",
        "plt.legend([\"mu_n\", \"pi\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comme on s'y attendait, on observe que $\\mu_n$ tend vers $\\pi$ lorsque n tend vers l'infini, soit ici lorsque n atteint des valeurs importantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "norm = np.linalg.norm(mu_n - pi, axis=1, ord=1) # Norme l1\n",
        "plt.title(\"Norme l1 de mu_n - pi\")\n",
        "plt.xlabel(\"n\")\n",
        "plt.ylabel(\"Norme\")\n",
        "plt.plot(norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Les observations ci-dessus sont confirmées par cette dernière courbe."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6\\. For each state $i \\in \\{1, \\dotsc, 5 \\}$, simulate 100 trajectories starting from the state $i$ until the return time to $i$. For each state, compute the (empirical) average return time. Compare with its theoretical value.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On définit dans le fichier `lab1_functions.py` une nouvelle fonction `simulate_dthmc_until_return` qui, au lieu d'arrêter la simulation au bout d'un nombre fini d'étapes, l'arrête quand on revient à l'état initial et renvoie le temps de retour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lab1_functions import simulate_dthmc_until_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulation de 100 trajectoires pour chaque état\n",
        "number_of_states = 6\n",
        "number_of_trajectories = 100\n",
        "\n",
        "mean_return_times = np.zeros(number_of_states)\n",
        "\n",
        "for i in range(number_of_states):\n",
        "    mu = np.zeros(number_of_states)\n",
        "    mu[i] = 1\n",
        "    return_times = np.zeros(number_of_trajectories)\n",
        "    for j in range(number_of_trajectories):\n",
        "        _, T = simulate_dthmc_until_return(P, mu, rng)\n",
        "        return_times[j] = T\n",
        "    mean_return_times[i] = np.mean(return_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La chaîne de Markov étant apériodique, irréductible et récurrente positive, on est dans les conditions d'application du théorème ergodique : \n",
        "\n",
        "$$\n",
        "\\forall i \\in \\mathcal S, \\pi_i = \\frac{1}{\\mathbb E(T^i)}\n",
        "$$\n",
        "\n",
        "Avec $\\pi$ la distribution limite de la chaîne de Markov.\n",
        "Nous allons donc comparer les temps de retours moyens empiriques aux valeurs de $\\frac{1}{\\pi_i}$ :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Les temps de retours moyens calculés pour chaque état sont : {mean_return_times}\")\n",
        "print(f\"Les espérances théoriques des temps de retour sont : {1/pi}\")\n",
        "print(f\"Ce qui correspond à un écart moyen de : {np.mean(mean_return_times - 1/pi)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ainsi, on retrouve bien des temps de retour moyens proches des valeurs théoriques : en moyenne, on a moins d'un pas de temps d'écart. Il faut aussi considérer le fait qu'on ne simule que 100 trajectoires : on aurait sans doute des résultats encore plus proches en augmentant ce nombre de trajectoires."
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "47a3d19953d0682b2fed03a7de16340ad054781b6349bd7c1cab4a5c5105088a"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('sdia-python': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
