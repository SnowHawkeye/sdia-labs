{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "### Noms et Prénoms du binome :\n",
        "- Hadrien SALEM\n",
        "- Emilie SALEM\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "# TP en Watermarking - TP2\n",
        "\n",
        "---\n",
        "\n",
        "## Patrick Bas, CNRS, CRIStAL\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Tatouage par également de spectre et attaques de sécurité\n",
        "\n",
        "### 2.1 Notations et rappels:\n",
        "\n",
        "Les notations sont identiques à celles vues en cours. Le procédé d'insertion est le schéma par étalement de spectre vu en cours.\n",
        "\n",
        "* X = matrice de $N_{i}$ vecteurs originaux de taille $N_{v}$ ($N_{i}$ colonnes, $N_{v}$ lignes). $N_{i}$ représente par exemple le nombre de contenu traités, et $N_{v}$ le nombre de composantes tatouées par contenu. **Note**: chaque colonne de X peut par exemple représenter des composantes d'une image.\n",
        "* $N_{o}$ représente le nombre de contenus tatoués observés par l'adversaire et utilisés pour construire son attaque\n",
        "* $N_{i}$ représente le nombre de contenus tatoués utilisés pour calculer pratiquement le taux d'erreur (voir BER)\n",
        "* Y = matrice de contenus tatoués\n",
        "* Z = matrice de contenus tatoués et perturbés\n",
        "* k clé secrète de norme unitaire\n",
        "* $m_{1}$: bit inséré, converti en +1, -1 \n",
        "* $\\alpha$: paramètre de distorsion\n",
        "* BER: Bit Error Rate, taux d'erreur binaire ou encore probabilité d'erreur empirique de décodage\n",
        "* DWR: « Document to Watermark Ratio » $DWR=10\\log_{10}(\\sum x_{i}^{2}/\\sum w_{i}^{2})$, permet de mesurer la distorsion ($DWR=0$ $\\Leftrightarrow$ $\\sigma_{X}^{2}=\\sigma_{W}^{2}$ ). Permet de mesurer la distortion. Distortion nulle $DWR=\\infty$, distortion importante $DWR \\rightarrow 0$ \n",
        "\n",
        "#### Rappels:\n",
        "* L'objectif du récepteur est de bien décoder $m_{1}$, possiblement en ayant une distortion qui ne soit pas trop importante\n",
        "* Ici, les objectifs de l'adversaire sont d'estimer la clé k puis d'effacer le message inséré. Pour s'assurer que l'adversaire a bien réussi à estimer la clé, il calculera la correlation normalisée entre le vecteur k et son estimation.\n",
        "\n",
        "**N'oubliez pas d'exporter votre TP en html lors de sa remise**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "%matplotlib inline  \n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import erf\n",
        "from sklearn.decomposition import FastICA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Scénario 1: Attaque à Messages connus, 1 bit\n",
        "* Mise en route: Quel est le BER cible de l'adversaire ?\n",
        "* Mettre en place l'attaque liée à ce scénario\n",
        "* Etudier l'impact de $N_{o}$ et de $\\alpha$ sur le BER après attaque"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Réponses \n",
        "\n",
        "- Le BER cible de l'adversaire est de $0.5$ : en effet, si le contenu n'est pas tatoué, on fait une erreur sur deux lors du décodage du message. Autrement dit, on ne peut plus retrouver le message inséré (puisque le message inséré a été effacé).\n",
        "- Voir code ci-dessous, en particulier l'implémentation de `hatk` en utilisant la formule : \n",
        "  $$\n",
        "  \\hat k_1 = \\frac{1}{\\alpha N_o} \\sum - (-1)^m y_i\n",
        "  $$\n",
        "- Comme on peut le voir sur la courbe \"`BER après attaque en fonction du nombre No d'observations utilisées`\", plus $N_o$ augmente plus le BER se rapproche de la valeur cible $0.5$. En effet, plus on fait d'observations, mieux on est capable d'estimer la valeur de $k$ (moyenne empirique) ce qui permet de retirer le message $m$ efficacement.\n",
        "- La courbe \"`BER après attaque en fonction du coefficient alpha`\" montre que plus le coefficient $\\alpha$ augmente, plus on semble se rapprocher et osciller autour de la valeur cible $0.5$. En effet, plus le coefficient de distorsion est important, plus il est facile de déterminer la direction correspondant à la clé (vecteurs bien polarisés)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "Nv = 100 # Size of the vector\n",
        "Ni = 10000 # Max number of observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "def ber(Y,m,k):# Compute the Bit Error Rate between message m and the extracted message from Y using key k\n",
        "    c = np.sign(np.dot(Y.T,k))\n",
        "    return np.sum(c != m)/np.float(Ni)\n",
        "\n",
        "def norm_corr(hatk,k): # Correlation between k and its estimate\n",
        "    hatk = hatk / np.sqrt(np.dot(hatk.T,hatk)) # Normalize\n",
        "    corrN = np.abs(np.dot(hatk.T,k))/(np.linalg.norm(hatk)*np.linalg.norm(k)) # Compute the Normalised correlation\n",
        "    return corrN\n",
        "\n",
        "def do_process(alpha,No):\n",
        "    \n",
        "    print('alpha: ',alpha)\n",
        "    print('No: ',No)\n",
        "\n",
        "    X = np.random.randn(Nv,Ni) # Generate Ni random host vectors\n",
        "    k = np.random.randn(Nv,1) # Generate the Watermark\n",
        "    k = k / np.sqrt(np.dot(k.T,k)) # Normalize the watermark\n",
        "\n",
        "    m1 = np.ones((Ni,1)) # Scenario with Known Messages: generate only ones, to be changed for the WOA attack!!!\n",
        "    K = np.dot(k,m1.T) # Generate the matrix of watermarks (each column contains m1_i*k)\n",
        "    W = alpha*K\n",
        "    Y = X + W # perform embedding\n",
        "    DWR = 10*np.log10(Nv/alpha**2) # Set the Document to Watermark Ratio, in dB\n",
        "    print('DWR: ',DWR,' dB')\n",
        "\n",
        "    cY = np.sign(np.dot(Y.T,k)) # Computation of the decoded 'bits' (here -1 or +1)\n",
        "    print('practical bit error rate:')\n",
        "    print(np.sum(cY != m1)/np.float(Ni)) \n",
        "\n",
        "    # Attack\n",
        "    Y_obs = Y[:,:No]\n",
        "    \n",
        "    hatk = np.zeros(Nv)\n",
        "    for v in range(Nv): # For each component of the key\n",
        "        for i in range(No): # Sum on observations\n",
        "            hatk[v] += -((-1)**m1[i])*Y_obs[v][i]/(alpha*No)\n",
        "    hatk = hatk / np.sqrt(np.dot(hatk.T,hatk)) # We need to Normalize\n",
        "    \n",
        "    corrN = np.abs(np.dot(hatk.T,k))/(np.linalg.norm(hatk)*np.linalg.norm(k)) # Compute the Normalised correlation\n",
        "    print('Normalised correlation between the true key and the estimated key')\n",
        "    print(corrN)\n",
        "\n",
        "    hatk = np.reshape(hatk,(Nv,1)) # We need to reshape\n",
        "\n",
        "    YA = Y - alpha*np.dot(hatk,m1.T) # KMA: perform the removal attack\n",
        "    practical_ber = ber(YA,m1,k)\n",
        "    print('practical bit error rate after security attack')\n",
        "    print(practical_ber)\n",
        "    print('\\n')\n",
        "    return practical_ber, corrN\n",
        "    \n",
        "\n",
        "alpha = 2 # Tune the power of the watermark here\n",
        "\n",
        "ber_list_No = []\n",
        "corrN_list_No = []\n",
        "No_list = np.arange(100,2000,100)\n",
        "\n",
        "for No in No_list:\n",
        "    practical_ber, corrN = do_process(alpha,No)\n",
        "    ber_list_No.append(practical_ber)\n",
        "    corrN_list_No.append(corrN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,7))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title(\"BER après attaque en fonction du nombre No d'observations utilisées\")\n",
        "plt.xlabel(\"Nombre d'observations No\")\n",
        "plt.ylabel(\"BER (en bits)\")\n",
        "plt.plot(No_list, ber_list_No)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(\"Corrélation normalisée après attaque en fonction du nombre No d'observations utilisées\")\n",
        "plt.xlabel(\"Nombre d'observations No\")\n",
        "plt.ylabel(\"Corrélation normalisée\")\n",
        "plt.plot(No_list, corrN_list_No)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "ber_list_alpha = []\n",
        "corrN_list_alpha = []\n",
        "alpha_list = np.arange(1,20)\n",
        "\n",
        "for alpha in alpha_list:\n",
        "    practical_ber, corrN_alpha = do_process(alpha,No=1000)\n",
        "    ber_list_alpha.append(practical_ber)\n",
        "    corrN_list_alpha.append(corrN_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,7))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title(\"BER après attaque en fonction du coefficient alpha\")\n",
        "plt.xlabel(\"Coefficient alpha\")\n",
        "plt.ylabel(\"BER (en bits)\")\n",
        "plt.plot(alpha_list, ber_list_alpha)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(\"Corrélation normalisée après attaque en fonction du nombre No d'observations utilisées\")\n",
        "plt.xlabel(\"Coefficient alpha\")\n",
        "plt.ylabel(\"Corrélation normalisée\")\n",
        "plt.plot(alpha_list, corrN_list_alpha)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Scénario 2, Attaque à messages inconnus, 1bit\n",
        "* Note: la fonction `np.linalg.eig` peut être utilisée pour effectuer une décomposition en valeurs et vecteurs propres.\n",
        "* Note: pour effacer le message inséré, il conviendra au préalable d'estimer le bit inséré, cela peut se faire via `m_est = np.sign(np.dot(Y.T,hatk))` où `hatk` est la clé estimée\n",
        "* Mettre en place l'attaque\n",
        "* Etudier l'impact de $N_{o}$ et de $\\alpha$ sur le BER après attaque"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Réponses\n",
        "\n",
        "* Encore une fois, plus on augmente le nombre d'observations $N_{o}$, plus on se rapproche de la valeur de BER cible $0.5$. Cependant, contrairement au premier scénario d'attaque, il faut plus d'observations pour s'en rapprocher, et même comme cela le résultat est moins bon (plus proche de $0.47$ que de $0.5$). En effet, on ne connaît plus le message ce qui le rend plus difficile à trouver, et nécessite d'inférer la direction de plus grande variance en se basant sur des observations.\n",
        "* De même, plus on augmente le coefficient $\\alpha$, plus le BER se rapproche de $0.5$ pour la même raison qu'au scénario précédent. En effet, lorsque la distortion est importante, les vecteurs sont très polarisés, ce qui rend plus facile la détermination de la direction de plus grande variance par la PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,7))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title(\"BER après attaque en fonction du nombre No d'observations utilisées\")\n",
        "plt.xlabel(\"Nombre d'observations No\")\n",
        "plt.ylabel(\"BER (en bits)\")\n",
        "plt.plot(No_list, ber_list_No)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(\"Corrélation normalisée après attaque en fonction du nombre No d'observations utilisées\")\n",
        "plt.xlabel(\"Nombre d'observations No\")\n",
        "plt.ylabel(\"Corrélation normalisée\")\n",
        "plt.plot(No_list, np.ravel(corrN_list_No))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "# 2dn scenario, scenario with KMA \n",
        "print('2nd Scenario')\n",
        "\n",
        "def do_process(alpha,No):\n",
        "    \n",
        "    print('alpha: ',alpha)\n",
        "    print('No: ',No)\n",
        "\n",
        "    X = np.random.randn(Nv,Ni) # Generate Ni random host vectors\n",
        "    k = np.random.randn(Nv,1) # Generate de Watermark\n",
        "    k = k / np.sqrt(np.dot(k.T,k)) # Normalize the watermark\n",
        "\n",
        "    m1 = np.sign(np.random.randn(Ni,1)) #Scenario with unknow messages, first bit\n",
        "\n",
        "    K = np.dot(k,m1.T) # Generate the matrix of watermarks (each column contains m1_i*k)\n",
        "\n",
        "    W = alpha*K\n",
        "\n",
        "    Y = X + W # perform embedding\n",
        "\n",
        "    # Attack\n",
        "    Y_obs = Y[:,:No]\n",
        "\n",
        "    cov = np.cov(Y_obs)\n",
        "    eigvals, eigvects = np.linalg.eig(cov)\n",
        "\n",
        "    idx = eigvals.argsort()[::-1]   \n",
        "    eigvals = eigvals[idx]\n",
        "    eigvects = eigvects[:,idx]\n",
        "    \n",
        "    hatk = eigvects[:,0]\n",
        "    hatk = np.reshape(hatk,(Nv,1)) # You might need to reshape the estimated key\n",
        "    corrN = norm_corr(hatk,k) # To ease the writing we use the norm_corr function\n",
        "    print(f'Normalised correlation between the true key and the estimated key: {corrN[0][0]}')\n",
        "\n",
        "    m_est = np.sign(np.dot(Y.T,hatk))\n",
        "    YA = Y - alpha*np.dot(hatk,m_est.T) # KMA: perform the removal attack\n",
        "\n",
        "    print(f'bit error rate after security attack: {ber(YA,m1,k)}')\n",
        "    print('\\n')\n",
        "    return practical_ber, corrN\n",
        "\n",
        "\n",
        "    \n",
        "alpha = 2 # Tune the power of the watermark here\n",
        "\n",
        "ber_list_No = []\n",
        "corrN_list_No = []\n",
        "No_list = np.arange(100,2000,100)\n",
        "\n",
        "for No in No_list:\n",
        "    practical_ber, corrN = do_process(alpha,No)\n",
        "    ber_list_No.append(practical_ber)\n",
        "    corrN_list_No.append(corrN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "ber_list_alpha = []\n",
        "corrN_list_alpha = []\n",
        "alpha_list = np.arange(1,20)\n",
        "\n",
        "for alpha in alpha_list:\n",
        "    practical_ber, corrN_alpha = do_process(alpha,No=1000)\n",
        "    ber_list_alpha.append(practical_ber)\n",
        "    corrN_list_alpha.append(corrN_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "ber_list_alpha = []\n",
        "corrN_list_alpha = []\n",
        "alpha_list = np.arange(1,20)\n",
        "\n",
        "for alpha in alpha_list:\n",
        "    practical_ber, corrN_alpha = do_process(alpha,No=1000)\n",
        "    ber_list_alpha.append(practical_ber)\n",
        "    corrN_list_alpha.append(corrN_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,7))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title(\"BER après attaque en fonction du coefficient alpha\")\n",
        "plt.xlabel(\"Coefficient alpha\")\n",
        "plt.ylabel(\"BER (en bits)\")\n",
        "plt.plot(alpha_list, ber_list_alpha)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(\"Corrélation normalisée après attaque en fonction du nombre No d'observations utilisées\")\n",
        "plt.xlabel(\"Coefficient alpha\")\n",
        "plt.ylabel(\"Corrélation normalisée\")\n",
        "plt.plot(alpha_list, np.ravel(corrN_list_alpha))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Scénario 3: Attaque à messages inconnus, 2 bits\n",
        "* Vérifier que l'attaque précédente ne permet pas d'estimer les deux clés. \n",
        "* Estimer au moins l'une des clés utilisée\n",
        "    * Note: on pourra utiliser l'algorithme `FastICA` pour estimer les deux composantes indépendantes \n",
        "    * Pour cela on pourra appeler la fonction fastica en spécifiant que l'analyse en composantes indépendantes s'effectuera sur un sous espace engendré par les *deux premières composantes principales* (`n_components=2`), et en récupérant les colonnes de la matrice de mélange A (obtenue via `ica.mixing_`) estimé par l'algorithme.\n",
        "    * Vérifier, à l'aide de la corrélation normalisée, que cette méthode permet d'estimer la clé `k1`\n",
        "* Mettre en place l'attaque qui permet d'effacter un bit sur les deux\n",
        "* Etudier l'impact de $N_{o}$ et de $\\alpha$ sur le BER après attaque"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Réponse\n",
        "\n",
        "* Nous avons essayé d'appliquer la méthode du scénario 2 (PCA) pour la détermination de la clé *k1*: on observe que les corrélations normalisées ont des valeurs avoisinant 70%, ce qui n'est pas du tout satisfaisant: l'attaque précédente ne permet donc pas d'estimer les deux clés.\n",
        "* Les courbes ci-dessous montrent qu'avec la méthode ICA, on est bien capable de d'estimer les deux composantes indépendantes. En particulier, pour la clé `k1`, on obtient des BER proches de 0.5 et des corrélations normalisées proches de 1, ce qui confirme l'efficacité de la méthode.\n",
        "* L'impact de $N_o$ et de $\\alpha$ est le même ici que pour les deux autres scénarios: le nombre d'observations améliore la qualité de la solution et le coefficient alpha rend plus simple la détermination des directions indépendantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "# 3rd scenario, 2 bits\n",
        "print('3rd scenario, 2bits')\n",
        "\n",
        "m1 = np.sign(np.random.randn(Ni,1))#Scenario with unknow messages, first bit\n",
        "m2 = np.sign(np.random.randn(Ni,1))#Scenario with unknow messages, second bit\n",
        "\n",
        "k1 = np.random.randn(Nv,1) # Generate de Watermark\n",
        "k1 = k1 / np.sqrt(np.dot(k1.T,k1)) # Normalize the watermark\n",
        "\n",
        "k2 = np.random.randn(Nv,1) # Generate de Watermark\n",
        "k2 = k2 / np.sqrt(np.dot(k2.T,k2)) # Normalize the watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "# 3rd scenario,n 2 bits, but applying method of scenario 2\n",
        "def do_process_with_old_method(alpha,No):\n",
        "\n",
        "    X = np.random.randn(Nv,Ni) # Generate Ni random host vectors\n",
        "    K = np.dot(k1,m1.T) + np.dot(k2,m2.T) # Generate the matrix of watermarks (each column contains m1_i*k)\n",
        "    W = alpha*K\n",
        "    Y = X + W # perform embedding\n",
        "    Y_obs = Y[:,:No]\n",
        "\n",
        "    cov = np.cov(Y_obs)\n",
        "    eigvals, eigvects = np.linalg.eig(cov)\n",
        "\n",
        "    idx = eigvals.argsort()[::-1]   \n",
        "    eigvals = eigvals[idx]\n",
        "    eigvects = eigvects[:,idx]\n",
        "    \n",
        "    hatk_1 = eigvects[:,0]\n",
        "    hatk_2 = eigvects[:,1]\n",
        "\n",
        "    hatk_1 = hatk_1/np.linalg.norm(hatk_1) # Normalize the vector\n",
        "    hatk_1 = np.reshape(hatk_1,(Nv,1)) # Reshape for upcoming comparisons\n",
        "\n",
        "    hatk_2 = hatk_2/np.linalg.norm(hatk_2)\n",
        "    hatk_2 = np.reshape(hatk_2,(Nv,1))\n",
        "\n",
        "    # It is uncertain which key hatk_1 and hatk_2 correspond to, so we need to test\n",
        "    corrN_v1 = norm_corr(hatk_1,k1) # Compute the Normalised correlation\n",
        "    corrN_v2 = norm_corr(hatk_1,k2)\n",
        "    \n",
        "    if(corrN_v1 < corrN_v2) : hatk_1, hatk_2 = hatk_2, hatk_1\n",
        "\n",
        "    corrN_1 = norm_corr(hatk_1,k1)\n",
        "    print(f'Normalised correlation between the estimated key and k1: {corrN_1[0][0]}')\n",
        "\n",
        "    corrN_2 = norm_corr(hatk_2,k2) # Compute the Normalised correlation\n",
        "    print(f'Normalised correlation between the estimated key and k2: {corrN_2[0][0]}')\n",
        "\n",
        "    m1_est = np.sign(np.dot(Y.T,hatk_1))\n",
        "    m2_est = np.sign(np.dot(Y.T,hatk_2))\n",
        "    \n",
        "    YA_1 = Y - alpha*np.dot(hatk_1,m1_est.T) # KMA: perform the removal attack\n",
        "    YA_2 = Y - alpha*np.dot(hatk_2,m2_est.T)\n",
        "    \n",
        "    ber1 = ber(YA_1,m1,k1)\n",
        "    ber2 = ber(YA_2,m2,k2)\n",
        "    print(f'bit error rate after security attack for the first bit: {ber1}')\n",
        "    print(f'bit error rate after security attack for the second bit: {ber2}')\n",
        "    print('\\n\\n')\n",
        "\n",
        "# One example, can be used to draw plots\n",
        "for No in range(100,2000,100):\n",
        "    do_process_with_old_method(alpha,No)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "def do_process(alpha,No):\n",
        "\n",
        "    print('alpha: ',alpha)\n",
        "    print('No: ',No)\n",
        "\n",
        "    X = np.random.randn(Nv,Ni) # Generate Ni random host vectors\n",
        "\n",
        "    K = np.dot(k1,m1.T) + np.dot(k2,m2.T) # Generate the matrix of watermarks (each column contains m1_i*k)\n",
        "\n",
        "    W = alpha*K\n",
        "\n",
        "    Y = X + W # perform embedding\n",
        "    \n",
        "    Y_obs = Y[:,:No]\n",
        "\n",
        "    ica = FastICA(n_components=2)\n",
        "    ica.fit(Y_obs.T)\n",
        "\n",
        "    hatk_1 = ica.mixing_[:,1]\n",
        "    hatk_2 = ica.mixing_[:,0]\n",
        "    \n",
        "    hatk_1 = hatk_1/np.linalg.norm(hatk_1) # Normalize the vector\n",
        "    hatk_1 = np.reshape(hatk_1,(Nv,1)) # Reshape for upcoming comparisons\n",
        "\n",
        "    hatk_2 = hatk_2/np.linalg.norm(hatk_2)\n",
        "    hatk_2 = np.reshape(hatk_2,(Nv,1))\n",
        "\n",
        "    # It is uncertain which key hatk_1 and hatk_2 correspond to, so we need to test\n",
        "    corrN_v1 = norm_corr(hatk_1,k1) # Compute the Normalised correlation\n",
        "    corrN_v2 = norm_corr(hatk_1,k2)\n",
        "    \n",
        "    if(corrN_v1 < corrN_v2) : hatk_1, hatk_2 = hatk_2, hatk_1\n",
        "\n",
        "    corrN_1 = norm_corr(hatk_1,k1)\n",
        "    print(f'Normalised correlation between the estimated key and k1: {corrN_1[0][0]}')\n",
        "\n",
        "    corrN_2 = norm_corr(hatk_2,k2) # Compute the Normalised correlation\n",
        "    print(f'Normalised correlation between the estimated key and k2: {corrN_2[0][0]}')\n",
        "\n",
        "    YA_1 = np.zeros((Nv,Ni)) # Perform the attack\n",
        "\n",
        "    m1_est = np.sign(np.dot(Y.T,hatk_1))\n",
        "    m2_est = np.sign(np.dot(Y.T,hatk_2))\n",
        "    \n",
        "    YA_1 = Y - alpha*np.dot(hatk_1,m1_est.T) # KMA: perform the removal attack\n",
        "    YA_2 = Y - alpha*np.dot(hatk_2,m2_est.T)\n",
        "    \n",
        "    ber1 = ber(YA_1,m1,k1)\n",
        "    ber2 = ber(YA_2,m2,k2)\n",
        "    print(f'bit error rate after security attack for the first bit: {ber1}')\n",
        "    print(f'bit error rate after security attack for the second bit: {ber2}')\n",
        "    print('\\n\\n')\n",
        "    # We only return the BER and correlation for the first bit\n",
        "    return ber1, corrN_1[0][0]\n",
        "\n",
        "alpha = 2 # Tune the power of the watermark here\n",
        "\n",
        "ber_list_No = []\n",
        "corrN_list_No = []\n",
        "No_list = np.arange(100,2000,100)\n",
        "\n",
        "# One example, can be used to draw plots\n",
        "for No in range(100,2000,100):\n",
        "    practical_ber, corrN = do_process(alpha,No)\n",
        "    ber_list_No.append(practical_ber)\n",
        "    corrN_list_No.append(corrN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,7))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title(\"BER du bit1 après attaque en fonction du nombre No d'observations utilisées\")\n",
        "plt.xlabel(\"Nombre d'observations No\")\n",
        "plt.ylabel(\"BER (en bits)\")\n",
        "plt.plot(No_list, ber_list_No)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(\"Corrélation normalisée du bit1 après attaque en fonction du nombre No d'observations utilisées\")\n",
        "plt.xlabel(\"Nombre d'observations No\")\n",
        "plt.ylabel(\"Corrélation normalisée\")\n",
        "plt.plot(No_list, np.ravel(corrN_list_No))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "ber_list_alpha = []\n",
        "corrN_list_alpha = []\n",
        "alpha_list = np.arange(1,20)\n",
        "\n",
        "for alpha in alpha_list:\n",
        "    practical_ber, corrN_alpha = do_process(alpha,No=1000)\n",
        "    ber_list_alpha.append(practical_ber)\n",
        "    corrN_list_alpha.append(corrN_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,7))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title(\"BER du bit1 après attaque en fonction du coefficient alpha\")\n",
        "plt.xlabel(\"Coefficient alpha\")\n",
        "plt.ylabel(\"BER (en bits)\")\n",
        "plt.plot(alpha_list, ber_list_alpha)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(\"Corrélation normalisée du bit1 après attaque en fonction du nombre No d'observations utilisées\")\n",
        "plt.xlabel(\"Coefficient alpha\")\n",
        "plt.ylabel(\"Corrélation normalisée\")\n",
        "plt.plot(alpha_list, corrN_list_alpha)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Pour conclure: Etude de la robustesse\n",
        "* Calculer le taux d'erreur (BER pour Bit Error Rate) théorique après ajout de bruit (voir cours)\n",
        "* Etudier l'évolution de la robustesse (via le BER) en fonction de la distortion $\\alpha$ \n",
        "* Quel compromis observe-t-on entre la sécurité et la robustesse?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "# Stéganalyse par apprentissage\n",
        "## Mise en route:\n",
        "* Récupèrer les caractéristiques ici: https://nextcloud.univ-lille.fr/index.php/s/i6xr4JykqAASapN\n",
        "* On charge les caractéristiques extraites à partir des images Cover et Stego pour d=3 (dimension de l'histogramme multivarié) et T=3 (seuil)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "cover = np.loadtxt('Features/cover-spam-N=3-T=3.csv')\n",
        "stego = np.loadtxt('Features/stego-0.20-lsb-spam-N=3-T=3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "print(cover.shape)\n",
        "print(stego.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "**Quelle est la dimension des caractéristiques ? Pourquoi ?**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "On a 10000 échantillons respectifs d'images cover et stégo contenant chacun les 686 bins d'un histogramme multivarié associé à une image. On a par conséquent 686 caractéristiques."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "**Entrainer un classifieur linéaire avec 5000 images en apprentissage et en test (effectuer une permutation pseudo-aléatoire des images avant l'apprentissage)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "# Creating the train set\n",
        "# Label 0 is for cover images and label 1 is for stego images\n",
        "cover_train = np.hstack([np.zeros(5000).reshape((5000,1)),cover[:5000,:]]) # We label 5000 cover images examples...\n",
        "stego_train = np.hstack([np.ones(5000).reshape((5000,1)),stego[:5000,:]]) # ... and 5000 stego images examples\n",
        "train = np.vstack([cover_train, stego_train]) # Cover and stego images are put together\n",
        "np.random.shuffle(train) # Shuffle the data\n",
        "\n",
        "# Separating train data and labels\n",
        "t_train = train[:, 0]\n",
        "data_train = train[:,1:]\n",
        "\n",
        "# Creating the test set\n",
        "cover_test = np.hstack([np.zeros(5000).reshape((5000,1)),cover[5000:,:]])\n",
        "stego_test = np.hstack([np.ones(5000).reshape((5000,1)),stego[5000:,:]])\n",
        "test = np.vstack([cover_test, stego_test])\n",
        "np.random.shuffle(test)\n",
        "\n",
        "# Separating test data and labels\n",
        "t_test = test[:,0]\n",
        "data_test = test[:,1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "# Discriminant Analysis (LDA linear classification, QDA for non-linear classification)\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "\n",
        "lda = LinearDiscriminantAnalysis().fit(data_train, t_train)\n",
        "qda = QuadraticDiscriminantAnalysis().fit(data_train, t_train)\n",
        "\n",
        "score_lda = lda.score(data_test, t_test)\n",
        "score_qda = qda.score(data_test, t_test)\n",
        "\n",
        "print(f\"Avec un classifieur linéaire (LDA), on obtient un score de {score_lda}.\")\n",
        "print(f\"Avec un classifieur non-linéaire (QDA), on obtient un score de {score_qda}.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## TODO: Commentaire"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "**Effectuer plusieurs entrainements/test successifs sur des ensembles d'apprentissage et de test différents (permutations différentes), commentez la variabilité**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "**Comparer avec les caractéristiques produites pour N = 2 et T = 4 (fournies), expliquer la différence de performance**"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "34ff3397c474938b265d2f4b45024e5465249fab18e07736c9a068b37b408800"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('python-all': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
