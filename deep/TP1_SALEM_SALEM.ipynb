{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 : Logistic regression by gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etudiant 1 : Emilie SALEM\n",
    "## Etudiant 2 : Hadrien SALEM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(nb_points):\n",
    "    \"\"\" Generates nb_points points uniformly in the unit square and assigns them a label with a randomized chance of being changed to the opposite class.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    for _ in range(nb_points):\n",
    "        x1, x2 = np.random.uniform(low = 0, high = 1, size = 2)\n",
    "        D = (0.5*x1 + x2 - 0.75) / np.sqrt(0.5**2 + 1) # distance between the generated point and the line\n",
    "        r = np.exp(-D**2/(2*0.05**2))\n",
    "        z = np.random.binomial(1, r/2) \n",
    "                   \n",
    "        if D > 0: label = 1 - z\n",
    "        else : label = 0 + z\n",
    "    \n",
    "        point = np.array([x1, x2])\n",
    "        data.append(point)\n",
    "        labels.append(label)\n",
    "    return np.array(data), np.array(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, labels = datagen(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We display points from class 0 and class 1 in different colors\n",
    "\n",
    "class0 = X[labels == 0]\n",
    "class1 = X[labels == 1]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(class0[:,0], class0[:,1], c='r')\n",
    "plt.scatter(class1[:,0], class1[:,1], c='b')\n",
    "plt.legend([\"Class 0\", \"Class 1\"])\n",
    "plt.title(\"Distribution des points dans les classes 0 et 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate X with a line of ones\n",
    "X_plus = np.hstack((np.ones((X.shape[0],1)), X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgm(x): return 1/(1+np.exp(-x)) # sigmoid function\n",
    "\n",
    "def gradient_descent(X_plus, labels, learn_rate, iter_max):\n",
    "    theta = np.random.rand(3)\n",
    "    history = [theta]\n",
    "    theta_new  = theta\n",
    "    N_iter = 0\n",
    "    \n",
    "    while(True): # Ensures that we go through the loop at least once\n",
    "        N_iter += 1\n",
    "        theta = theta_new\n",
    "        pred = sgm(X_plus.dot(theta)).reshape(len(X_plus),)\n",
    "        err = pred - labels \n",
    "        g = X_plus.T.dot(err)\n",
    "        theta_new = theta - learn_rate*g\n",
    "        history.append(theta_new)\n",
    "        \n",
    "        if not ((np.linalg.norm(theta_new-theta) > 10e-8) and (N_iter < iter_max)): break\n",
    "        \n",
    "    return history[-1], np.array(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_descent(thetas) :\n",
    "    \"\"\" Displays a 3D representation of the elements of thetas over the iterations of the gradient descent.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=((7,7)))\n",
    "    ax = fig.gca(projection=\"3d\")\n",
    "    ax.plot(thetas[:,0],thetas[:,1],thetas[:,2],\"-o\")\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec un learning rate de 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_desc_1, thetas_desc_1 = gradient_descent(X_plus, labels, 0.02, 50000)\n",
    "print(\"Nombre d'itérations : \" + str(len(thetas_desc_1)))\n",
    "display_descent(thetas_desc_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec un learning rate de 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_desc_2, thetas_desc_2 = gradient_descent(X_plus, labels, 0.1, 50000)\n",
    "print(\"Nombre d'itérations : \" + str(len(thetas_desc_2)))\n",
    "display_descent(thetas_desc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire\n",
    "\n",
    "On observe qu'avec un learning rate plus important, on fait des pas de gradient plus grands : il y a plus d'oscillations et on risque de ne pas converger, mais si on converge effectivement la solution est atteinte bien plus vite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X_plus, labels, learn_rate, epoch_max):\n",
    "    theta = np.random.rand(3)\n",
    "    history = [theta]\n",
    "    theta_new = theta\n",
    "    N_epoch = 0\n",
    "    \n",
    "    while(True):\n",
    "        N_epoch += 1\n",
    "        if(N_epoch%10 == 0): print(f\"...Processing epoch : {N_epoch}\")\n",
    "        \n",
    "        # Permutation of data\n",
    "        data_and_labels = np.hstack([X_plus, labels.reshape(len(X_plus),1)])\n",
    "        data_and_labels = np.random.permutation(data_and_labels)\n",
    "        \n",
    "        X_plus_shuffled = data_and_labels[:,:3]\n",
    "        labels_shuffled = data_and_labels[:,3]\n",
    "        \n",
    "        for i in range(len(X_plus_shuffled)) :\n",
    "            theta = theta_new\n",
    "            pred = sgm(X_plus_shuffled[i].dot(theta))\n",
    "            err = pred - labels_shuffled[i] \n",
    "            g = X_plus_shuffled[i]*err\n",
    "            theta_new = theta - learn_rate*g\n",
    "        \n",
    "        history.append(theta_new)\n",
    "        \n",
    "        if(not((np.linalg.norm(theta_new-theta) > 10e-8) and (N_epoch < epoch_max))): break\n",
    "        \n",
    "    return history[-1], np.array(history), N_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_stoch, thetas_stoch, epochs_stoch = stochastic_gradient_descent(X_plus, labels, 1.5, 100)\n",
    "print(f\"Nombre d'epochs : {epochs_stoch}.\")\n",
    "display_descent(thetas_stoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_figure(theta):\n",
    "    plt.scatter(class0[:,0], class0[:,1], c='r')\n",
    "    plt.scatter(class1[:,0], class1[:,1], c='b')\n",
    "\n",
    "    x1 = np.linspace(0,1,10) \n",
    "    x2 = (-theta[0]-theta[1]*x1)/theta[2]\n",
    "    plt.plot(x1,x2,'k--')\n",
    "\n",
    "    plt.legend([\"Frontière de décision\", \"Class 0\", \"Class 1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "plt.subplot(131)\n",
    "boundary_figure(theta_desc_1)\n",
    "plt.title(f\"Descente de gradient avec learning rate de 0.02 \\n({len(thetas_desc_1)} iterations)\")\n",
    "\n",
    "\n",
    "plt.subplot(132)\n",
    "boundary_figure(theta_desc_2)\n",
    "plt.title(f\"Descente de gradient avec learning rate de 0.1 \\n({len(thetas_desc_2)} iterations)\")\n",
    "\n",
    "\n",
    "plt.subplot(133)\n",
    "boundary_figure(theta_stoch)\n",
    "plt.title(f\"Descente de gradient stochastique avec learning rate de 1.5 \\n({epochs_stoch} epochs)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire\n",
    "\n",
    "On remarque que la méthode de gradient stochastique converge beaucoup plus vite, puisqu'une epoch de gradient stochastique correspond à une itération de gradient classique en termes de nombre de calculs. Cependant, il y a généralement plus de \"bruit\" au cours des epochs, ce qui peut donner des solutions de moins bonne qualité. En particulier, la frontière de décision semble parfois légèrement biaisée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_batch(X_plus, labels, learn_rate, epoch_max, batch_size, epsilon):\n",
    "    theta = np.random.rand(3)\n",
    "    history = [theta]\n",
    "    theta_new = theta\n",
    "    N_epoch = 0\n",
    "    \n",
    "    while(True):\n",
    "        N_epoch += 1\n",
    "        if(N_epoch%10 == 0): print(f\"...Processing epoch : {N_epoch}\")\n",
    "        \n",
    "        # Permutation of data\n",
    "        data_and_labels = np.hstack([X_plus, labels.reshape(len(X_plus),1)])\n",
    "        data_and_labels = np.random.permutation(data_and_labels)\n",
    "        \n",
    "        X_plus_shuffled = data_and_labels[:,:3]\n",
    "        labels_shuffled = data_and_labels[:,3]\n",
    "        \n",
    "        N = len(X_plus_shuffled)\n",
    "        \n",
    "        for i in range(0, N, batch_size) :\n",
    "            k = min(batch_size, N - i) # manages the case where there is not enough remaining data to fill a batch \n",
    "                        \n",
    "            theta = theta_new\n",
    "            pred = sgm(X_plus_shuffled[i:i+k].dot(theta)).reshape(k,)\n",
    "            err = pred - labels_shuffled[i:i+k] \n",
    "            g = X_plus_shuffled[i:i+k].T.dot(err)\n",
    "            theta_new = theta - learn_rate*g\n",
    "        \n",
    "        history.append(theta_new)\n",
    "        \n",
    "        if(not((np.linalg.norm(theta_new-theta) > epsilon) and (N_epoch < epoch_max))): break\n",
    "        \n",
    "    print(np.linalg.norm(theta_new-theta))\n",
    "        \n",
    "    return history[-1], np.array(history), N_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_batch, thetas_batch, epochs = sgd_batch(X_plus, labels, learn_rate = 0.02, epoch_max = 1000, batch_size = 10, epsilon = 10e-4)\n",
    "print(f\"Nombre d'epochs : {epochs}.\")\n",
    "display_descent(thetas_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire\n",
    "\n",
    "On remarque qu'avec la méthode de mini-batch, plus d'epochs sont nécessaires pour converger. En contrepartie, on réduit les oscillations lors de la descente de gradient."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9963137a101e18cad171261b7fa1fb03f34a5ba10ba22de7f6ebf9ccd0228ae6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('python-all': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
