{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# if the import fails, try to install tf : pip install --upgrade tensorflow\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy.random as rnd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rep = \".\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "# Exercice 1: Comparision with PCA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Dataset generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "rnd.seed(4)\n",
        "m = 200\n",
        "w1, w2 = 0.1, 0.3\n",
        "noise = 0.1\n",
        "\n",
        "angles = rnd.rand(m) * 3 * np.pi / 2 - 0.5\n",
        "data = np.empty((m, 3))\n",
        "data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * rnd.randn(m) / 2\n",
        "data[:, 1] = np.sin(angles) * 0.7 + noise * rnd.randn(m) / 2\n",
        "data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * rnd.randn(m)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(data[:100]).astype('float32')\n",
        "X_test = scaler.transform(data[100:]).astype('float32')\n",
        "\n",
        "fig = plt.figure(figsize=(10,25))\n",
        "ax = fig.gca(projection='3d')\n",
        "ax.plot(X_train[:,0], X_train[:,1], X_train[:,2],'.', label='dataset')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Dimensionality reduction with an auto-encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "d_in = 3 #input dimensionality\n",
        "d_hid = 2 #code dimensionality\n",
        "d_out = d_in #output dimensionality\n",
        "learning_rate = 0.1\n",
        "\n",
        "activation = tf.nn.elu\n",
        "\n",
        "class basic_AE(tf.Module):\n",
        "    def __init__(self, unit_nbrs, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.w1 = tf.Variable(tf.random.normal([d_in, d_hid]), name='w')\n",
        "        self.b1 = tf.Variable(tf.zeros([d_hid]), name='b1')\n",
        "        self.b2 = tf.Variable(tf.zeros([d_in]), name='b2')\n",
        "        self.K = len(unit_nbrs)-1\n",
        "\n",
        "    @tf.function\n",
        "    def __call__(self, x):   \n",
        "        z = activation(tf.matmul(x,self.w1) + self.b1)\n",
        "        x_tilde = activation(tf.matmul(z,tf.transpose(self.w1)) + self.b2) \n",
        "        return x_tilde\n",
        "\n",
        "def loss(target,pred):\n",
        "    return tf.math.reduce_mean(tf.math.squared_difference(target, pred))  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "### Model creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "mini_ae = basic_AE([d_in, d_hid],name=\"first_ae\")\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "n_epochs = 500\n",
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Computing the function meanwhile recording a gradient tape\n",
        "    with tf.GradientTape() as tape: # tape: records gradients throughout iterations\n",
        "        train_loss = loss(X_train,mini_ae(X_train))\n",
        "    train_loss_history.append(train_loss)\n",
        "    test_loss_history.append(loss(X_test,mini_ae(X_test)))\n",
        "    grads = tape.gradient(train_loss,mini_ae.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, mini_ae.trainable_variables))\n",
        "    print(\"Epoch %d  - \\tf=%s\" % (epoch, train_loss.numpy()), end=\"\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "codings_val = activation(tf.matmul(X_train,mini_ae.w1) + mini_ae.b1)\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.plot(codings_val[:,0], codings_val[:, 1], \"b.\")\n",
        "plt.xlabel(\"$z_1$\", fontsize=12)\n",
        "plt.ylabel(\"$z_2$\", fontsize=12, rotation=0)\n",
        "plt.title('Dim. Red. with AE')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.plot(train_loss_history)\n",
        "plt.plot(test_loss_history)\n",
        "plt.title(\"Evolution of train and test loss throughout epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"train_loss\", \"test_loss\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "### PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "# PCA from scikit-learn\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "\n",
        "singval = pca.singular_values_   # eigenvalues\n",
        "comp = pca.components_           # principal components\n",
        "proj = pca.transform(X_train)    # computes the projection coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "proj_pca = np.dot(proj, comp[0:2,:].T)\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.plot(proj_pca[:,0], proj_pca[:, 1], \"b.\")\n",
        "plt.xlabel(\"$pca_1$\", fontsize=12)\n",
        "plt.ylabel(\"$ca_2$\", fontsize=12, rotation=0)\n",
        "plt.title('Dim. Red. with PCA')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## TODO: COMMENT ALL QUESTIONS!!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "# Exercise 2: AE for representation learning with MNIST"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Dataset import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "n = x_train.shape[0]\n",
        "d_inputs = 28 * 28\n",
        "d_hidden1 = 100\n",
        "d_hidden2 = 10  # codings\n",
        "d_hidden3 = d_hidden1\n",
        "d_outputs = d_inputs\n",
        "n_class = 10\n",
        "\n",
        "learning_rate = 1e-1\n",
        "l2_reg = 0.0005\n",
        "batch_size = 10\n",
        "steps = n//batch_size\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    learning_rate,\n",
        "    decay_steps=500,\n",
        "    decay_rate=0.96)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Dataset formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape((x_train.shape[0],x_train.shape[1]*x_train.shape[2]))/255 - 0.5\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.reshape((x_test.shape[0],x_test.shape[1]*x_test.shape[2]))/255 - 0.5\n",
        "x_test = x_test.astype('float32')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "class AE(tf.Module):\n",
        "    def __init__(self, unit_nbrs, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.w = []\n",
        "        self.b = []\n",
        "        self.K = len(unit_nbrs)-1\n",
        "\n",
        "        for i in range(self.K): \n",
        "            self.w.append(tf.Variable(tf.random.normal([unit_nbrs[i], unit_nbrs[i+1]], name='w')))\n",
        "            self.b.append(tf.Variable(tf.zeros([unit_nbrs[i+1]]), name='b'+ str(i+1)))\n",
        "\n",
        "        for i in range(self.K-1):  \n",
        "            self.b.append(tf.Variable(tf.zeros([self.w[-i].shape[1]]), name='b' + str(self.K) + str(i)))\n",
        "\n",
        "        self.b.append(tf.Variable(tf.zeros(unit_nbrs[0]), name= 'b' + str(len(self.b)+1)))\n",
        "        \n",
        "    @tf.function\n",
        "    def __call__(self, x):\n",
        "        z = [activation(tf.matmul(x,self.w[0]) + self.b[0])]\n",
        "\n",
        "        for i in range(1, self.K):  \n",
        "            z.append(activation(tf.matmul(z[i-1],self.w[i]) + self.b[i]))\n",
        "\n",
        "        for i in range(0, self.K):\n",
        "            z.append(activation(tf.matmul(z[i-1],tf.transpose(self.w[self.K-i-1])) + self.b[self.K+i]))\n",
        "\n",
        "        return z[-1]\n",
        "    \n",
        "def loss(target,pred):\n",
        "    return tf.math.reduce_mean(tf.math.squared_difference(target, pred))  \n",
        "\n",
        "def reg(model,l2_reg):\n",
        "    term = 0\n",
        "    for coef in model.trainable_variables:\n",
        "        if (coef.name[0]=='w'):\n",
        "            term += coef^2\n",
        "    return l2_reg*term"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Model creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "my_AE = AE([d_inputs,d_hidden1,d_hidden2], name=\"the_model\")\n",
        "print(\"Model results:\", my_AE(x_train[0:2]))\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "n_epochs = 4\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for step in range(steps):\n",
        "        # Computing the function meanwhile recording a gradient tape\n",
        "        with tf.GradientTape() as tape: \n",
        "            train_loss = loss(x_train,my_AE(x_train)) + reg(my_AE, l2_reg)\n",
        "\n",
        "        grads = tape.gradient(train_loss,my_AE.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, my_AE.trainable_variables))\n",
        "        print(\"\\rEpoch %d - %d%% - \\tf=%s\" % (epoch, int(step/steps*100), train_loss.numpy()),end=\"\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "source": [
        "## Call with random results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": [
        "ind = 1000\n",
        "print(\"Inputs:\", x_train[ind:ind+1][0,:5])\n",
        "print(\"Model results:\", my_AE(x_train[ind:ind+1])[0,:5])\n",
        "x_tilde = my_AE(x_train[ind:ind+1]).numpy()\n",
        "\n",
        "plt.imshow(np.reshape(x_tilde,(28,28)), cmap='gray', interpolation=\"nearest\")\n",
        "\n",
        "x_tilde_test = my_AE(x_test)\n",
        "test_loss = loss(x_test,x_tilde_test)\n",
        "print(\"Test MSE =\",test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "datalore": {
          "sheet_delimiter": false
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "34ff3397c474938b265d2f4b45024e5465249fab18e07736c9a068b37b408800"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('python-all': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
