{
  "cells":[
    {
      "cell_type":"code",
      "source":[
        "import tensorflow as tf\n",
        "# if the import fails, try to install tf : pip install --upgrade tensorflow\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy.random as rnd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rep = \".\""
      ],
      "execution_count":143,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Exercice 1: Comparision with PCA"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Dataset generation"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "rnd.seed(4)\n",
        "m = 200\n",
        "w1, w2 = 0.1, 0.3\n",
        "noise = 0.1\n",
        "\n",
        "angles = rnd.rand(m) * 3 * np.pi \/ 2 - 0.5\n",
        "data = np.empty((m, 3))\n",
        "data[:, 0] = np.cos(angles) + np.sin(angles)\/2 + noise * rnd.randn(m) \/ 2\n",
        "data[:, 1] = np.sin(angles) * 0.7 + noise * rnd.randn(m) \/ 2\n",
        "data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * rnd.randn(m)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(data[:100]).astype('float32')\n",
        "X_test = scaler.transform(data[100:]).astype('float32')\n",
        "\n",
        "fig = plt.figure(figsize=(10,25))\n",
        "ax = fig.gca(projection='3d')\n",
        "ax.plot(X_train[:,0], X_train[:,1], X_train[:,2],'.', label='dataset')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count":144,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Dimensionality reduction with an auto-encoder"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "d_in = 3 #input dimensionality\n",
        "d_hid = 2 #code dimensionality\n",
        "d_out = d_in #output dimensionality\n",
        "learning_rate = 0.1\n",
        "\n",
        "activation = tf.nn.elu\n",
        "\n",
        "class basic_AE(tf.Module):\n",
        "    def __init__(self, unit_nbrs, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.w1 = tf.Variable(tf.random.normal([d_in, d_hid]), name='w')\n",
        "        self.b1 = tf.Variable(tf.zeros([d_hid]), name='b1')\n",
        "        self.b2 = tf.Variable(tf.zeros([d_in]), name='b2')\n",
        "        self.K = len(unit_nbrs)-1\n",
        "\n",
        "    @tf.function\n",
        "    def __call__(self, x):   \n",
        "        z = activation(tf.matmul(x,self.w1) + self.b1)\n",
        "        x_tilde = activation(tf.matmul(z,tf.transpose(self.w1)) + self.b2) \n",
        "        return x_tilde\n",
        "\n",
        "def loss(target,pred):\n",
        "    return tf.math.reduce_mean(tf.math.squared_difference(target, pred))  "
      ],
      "execution_count":145,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Model creation"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "mini_ae = basic_AE([d_in, d_hid],name=\"first_ae\")\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)"
      ],
      "execution_count":146,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Training"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "n_epochs = 500\n",
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Computing the function meanwhile recording a gradient tape\n",
        "    with tf.GradientTape() as tape: # tape: records gradients throughout iterations\n",
        "        train_loss = loss(X_train,mini_ae(X_train))\n",
        "    train_loss_history.append(train_loss)\n",
        "    test_loss_history.append(loss(X_test,mini_ae(X_test)))\n",
        "    grads = tape.gradient(train_loss,mini_ae.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, mini_ae.trainable_variables))\n",
        "    print(\"Epoch %d  - \\tf=%s\" % (epoch, train_loss.numpy()), end=\"\")"
      ],
      "execution_count":147,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Plots"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "codings_val = activation(tf.matmul(X_train,mini_ae.w1) + mini_ae.b1)\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.plot(codings_val[:,0], codings_val[:, 1], \"b.\")\n",
        "plt.xlabel(\"$z_1$\", fontsize=12)\n",
        "plt.ylabel(\"$z_2$\", fontsize=12, rotation=0)\n",
        "plt.title('Dim. Red. with AE')\n",
        "plt.show()"
      ],
      "execution_count":148,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.plot(train_loss_history)\n",
        "plt.plot(test_loss_history)\n",
        "plt.title(\"Evolution of train and test loss throughout epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"train_loss\", \"test_loss\"])"
      ],
      "execution_count":149,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### PCA"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# PCA from scikit-learn\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "\n",
        "singval = pca.singular_values_   # eigenvalues\n",
        "comp = pca.components_           # principal components\n",
        "proj = pca.transform(X_train)    # computes the projection coefficients"
      ],
      "execution_count":164,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "proj_pca = np.dot(proj, comp[0:2,:].T)\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.plot(proj_pca[:,0], proj_pca[:, 1], \"b.\")\n",
        "plt.xlabel(\"$pca_1$\", fontsize=12)\n",
        "plt.ylabel(\"$ca_2$\", fontsize=12, rotation=0)\n",
        "plt.title('Dim. Red. with PCA')\n",
        "plt.show()"
      ],
      "execution_count":165,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## TODO: COMMENT ALL QUESTIONS!!"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Exercise 2: AE for representation learning with MNIST"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Dataset import"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count":166,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Constants"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "n = x_train.shape[0]\n",
        "d_inputs = 28 * 28\n",
        "d_hidden1 = 100\n",
        "d_hidden2 = 10  # codings\n",
        "d_hidden3 = d_hidden1\n",
        "d_outputs = d_inputs\n",
        "n_class = 10\n",
        "\n",
        "learning_rate = 1e-1\n",
        "l2_reg = 0.0005\n",
        "batch_size = 10\n",
        "steps = n\/\/batch_size\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    learning_rate,\n",
        "    decay_steps=500,\n",
        "    decay_rate=0.96)"
      ],
      "execution_count":167,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Dataset formatting"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "x_train = x_train.reshape((x_train.shape[0],x_train.shape[1]*x_train.shape[2]))\/255 - 0.5\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.reshape((x_test.shape[0],x_test.shape[1]*x_test.shape[2]))\/255 - 0.5\n",
        "x_test = x_test.astype('float32')"
      ],
      "execution_count":168,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Model definition"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "class AE(tf.Module):\n",
        "    def __init__(self, unit_nbrs, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.w = []\n",
        "        self.b = []\n",
        "        self.K = len(unit_nbrs)-1\n",
        "        ## TODO: dimensions\n",
        "        for i in range(self.K):            \n",
        "            self.w.append(tf.Variable(tf.random.normal([unit_nbrs[i], unit_nbrs[i+1]], name='w')))\n",
        "            self.b.append(tf.Variable(tf.zeros([unit_nbrs[i+1]]), name='b'+ str(i+1)))\n",
        "        for i in range(self.K):            \n",
        "            self.b.append(tf.Variable(tf.zeros([unit_nbrs[-i+1]]), name='b' + str(self.K) + str(i)))  \n",
        "        \n",
        "    @tf.function\n",
        "    def __call__(self, x):\n",
        "        z = [x]\n",
        "        for i in range(self.K):  \n",
        "            z.append(activation(tf.matmul(x,self.w[i]) + self.b[i]))\n",
        "        for i in range(self.K):  \n",
        "            z.append(activation(tf.matmul(z,tf.transpose(self.w[i])) + self.b[i+self.K]))\n",
        "        return z[-1]\n",
        "    \n",
        "def loss(target,pred):\n",
        "    return tf.math.reduce_mean(tf.math.squared_difference(target, pred))  \n",
        "\n",
        "def reg(model,l2_reg):\n",
        "    term = 0\n",
        "    for coef in model.trainable_variables:\n",
        "        if (coef.name[0]=='w'):\n",
        "            term += ...\n",
        "    return l2_reg*term"
      ],
      "execution_count":186,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Model creation"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "my_AE = AE([d_inputs,d_hidden1,d_hidden2], name=\"the_model\")\n",
        "print(\"Model results:\", my_AE(x_train[0:2]))\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
      ],
      "execution_count":187,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Training"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "n_epochs = 4\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for step in range(steps):\n",
        "        # Computing the function meanwhile recording a gradient tape\n",
        "        with tf.GradientTape() as tape: \n",
        "            ...\n",
        "            train_loss = ...\n",
        "\n",
        "        grads = tape.gradient(train_loss,my_AE.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, my_AE.trainable_variables))\n",
        "        print(\"\\rEpoch %d - %d%% - \\tf=%s\" % (epoch, int(step\/steps*100), train_loss.numpy()),end=\"\")"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "## Call with random results"
      ],
      "attachments":{
        
      },
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    },
    {
      "cell_type":"code",
      "source":[
        "ind = 1000\n",
        "print(\"Inputs:\", x_train[ind:ind+1][0,:5])\n",
        "print(\"Model results:\", my_AE(x_train[ind:ind+1])[0,:5])\n",
        "x_tilde = my_AE(x_train[ind:ind+1]).numpy()\n",
        "\n",
        "plt.imshow(np.reshape(x_tilde,(28,28)), cmap='gray', interpolation=\"nearest\")\n",
        "\n",
        "x_tilde_test = my_AE(x_test)\n",
        "test_loss = loss(x_test,x_tilde_test)\n",
        "print(\"Test MSE =\",test_loss)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        "datalore":{
          "sheet_delimiter":false
        }
      }
    }
  ],
  "metadata":{
    
  },
  "nbformat":4,
  "nbformat_minor":0
}