{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep learning - Practical nÂ°2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Student 1 : Emilie SALEM\n",
        "## Student 2 : Hadrien SALEM "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_circles\n",
        "import matplotlib.pyplot as plt\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generation and display of the dataset\n",
        "\n",
        "X,y = make_circles(n_samples = 300, noise=0.2, factor=0.1)\n",
        "n,d = X.shape\n",
        "plt.figure(1,figsize=[6,6])\n",
        "plt.plot(X[:,0][np.where(y==1)],X[:,1][np.where(y==1)],\".c\")\n",
        "plt.plot(X[:,0][np.where(y==0)],X[:,1][np.where(y==0)],\".m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions\n",
        "\n",
        "def sigmoid(x):\n",
        "    res = 1 / (1 + np.exp(-x))\n",
        "    return res\n",
        "\n",
        "def onehot(y,i):\n",
        "    \"\"\"Creates a one-hot encoding of the i element of y\"\"\"\n",
        "    return 1-np.array(list(bin(y[i]+1)[2:].zfill(2))).astype(int)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 1: Network creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definition of an input unit\n",
        "class InputUnit:\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,data):\n",
        "        self.data = data # one column of matrix X\n",
        "        self.n = data.shape[0] # dataset size\n",
        "        self.k = 0 # layer number\n",
        "        self.z = 0 # unit output\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Input unit in layer {self.k}, containing {self.n} examples.\"\n",
        "\n",
        "    def plug(self, unit):\n",
        "        unit.preceding.append(self)\n",
        "\n",
        "    def forward(self,i):\n",
        "        self.z = self.data[i] \n",
        "        return self.z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creation of 2 input units, one for each column of X\n",
        "input_unit_1 = InputUnit(X[:,0])\n",
        "input_unit_2 = InputUnit(X[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definition of a neural unit\n",
        "class NeuralUnit:\n",
        "    \n",
        "    # Constructor\n",
        "    def __init__(self,k,u):\n",
        "        self.u = u # unit number\n",
        "        self.preceding = [] # list of preceding neurons\n",
        "        # self.npr = 0 # length of list preceding\n",
        "        self.following = [] # list of following neurons\n",
        "        # self.nfo = 0 # length of list following\n",
        "        self.k = k # layer number\n",
        "        self.w = 0 # unit weights\n",
        "        self.b = 0 # unit intercept\n",
        "        self.z = 0 # unit output\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Neural unit #{self.u} in layer {self.k} with {self.npr} preceding element(s) and {self.nfo} following element(s).\"\n",
        "\n",
        "    @property # custom getter for length of list preceding\n",
        "    def npr(self):\n",
        "        return len(self.preceding)\n",
        "\n",
        "    @property # custom getter for length of list following\n",
        "    def nfo(self):\n",
        "        return len(self.following)\n",
        "\n",
        "    def reset_params(self):\n",
        "        self.w = np.random.randn(self.npr)\n",
        "        self.b = np.random.randn()\n",
        "\n",
        "    def plug(self, unit):\n",
        "        unit.preceding.append(self)\n",
        "        self.following.append(unit)\n",
        "\n",
        "    def forward(self, i):\n",
        "        z_in = np.zeros(self.npr)\n",
        "        for index, unit in enumerate(self.preceding) :\n",
        "            z_in[index] = unit.forward(i)\n",
        "        self.z = sigmoid(self.w.dot(z_in) + self.b) # ! transpose ? reshape ?\n",
        "        return self.z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creation of the 3 neural units corresponding to this problem\n",
        "\n",
        "# Layer 1\n",
        "neural_unit_11 = NeuralUnit(1, 1)\n",
        "neural_unit_12 = NeuralUnit(1, 2)\n",
        "\n",
        "# Layer 2\n",
        "neural_unit_21 = NeuralUnit(2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Loss:\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,y,k):\n",
        "        self.preceding = [] # list of preceding neurons\n",
        "        # self.npr = 0 # length of list preceding\n",
        "        self.y = y # array of class labels of the training data\n",
        "        self.k = k # layer index\n",
        "            \n",
        "    def __str__(self):\n",
        "        return f\"Loss unit in layer {self.k} with {self.npr} preceding element(s).\"\n",
        "\n",
        "    @property # custom getter for length of list preceding\n",
        "    def npr(self):\n",
        "        return len(self.preceding)\n",
        "\n",
        "    def forward(self, i):\n",
        "        z_in = self.preceding[0].forward(i)\n",
        "        if self.y[i] == 0 : return -np.log(1-z_in)\n",
        "        else : return -np.log(z_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creation of the loss unit\n",
        "loss_unit = Loss(y, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    # Constructor\n",
        "    def __init__(self,X,y,archi):\n",
        "        self.archi = archi\n",
        "        self.X = X\n",
        "        self.n = X.shape[0]\n",
        "        self.y = y\n",
        "        self.K = len(archi) # number of layers (including the input layer but without the loss layer)\n",
        "\n",
        "        # Creating network\n",
        "        net = []\n",
        "\n",
        "        # Inputs\n",
        "        n_inputs = archi[0]\n",
        "        input_layer = []\n",
        "        for i in range(n_inputs):\n",
        "            input_layer.append(InputUnit(X[:,i]))\n",
        "        net.append(input_layer)\n",
        "            \n",
        "        # Neural units\n",
        "        for k in range(1, self.K) :\n",
        "            layer = []\n",
        "            n_units = archi[k]\n",
        "            for j in range(1, n_units + 1):\n",
        "                layer.append(NeuralUnit(k,j))\n",
        "            net.append(layer)\n",
        "\n",
        "        # Loss\n",
        "        loss_layer = [Loss(y, self.K)]\n",
        "        net.append(loss_layer)\n",
        "\n",
        "        # Plugging units together and resetting parameters\n",
        "        for k in range(len(net) - 1):\n",
        "            for unit in net[k] :\n",
        "                if isinstance(unit, NeuralUnit) : unit.reset_params()\n",
        "                for next_unit in net[k+1]:\n",
        "                    unit.plug(next_unit)\n",
        "\n",
        "        self.net = net\n",
        "\n",
        "    def forward(self,i):\n",
        "        return self.net[-1][0].forward(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instanciation of the network corresponding to our problem\n",
        "archi = [2,2,1]\n",
        "mlp = MLP(X,y,archi)\n",
        "\n",
        "# Check unit connections and parameters initialization\n",
        "for layer in mlp.net :\n",
        "    for unit in layer :\n",
        "        print(unit)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 2: Forward pass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We programmed the `forward(self,i)` functions in all the above classes (different types of units and `MLP`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checking that the forward methods are correctly programmed\n",
        "\n",
        "index_0 = 0 # index of an element for which y = 0\n",
        "loss_0 = mlp.forward(index_0)\n",
        "output_neural_0 = mlp.net[2][0].z # output of the last neural unit of the network\n",
        "print(f\"For an example of class 0, the output of the last neural unit is {output_neural_0} and the loss is {loss_0}.\")\n",
        "\n",
        "\n",
        "index_1 = 45 # index of an element for which y = 1\n",
        "loss_1 = mlp.forward(index_1)\n",
        "output_neural_1 = mlp.net[2][0].z\n",
        "print(f\"For an example of class 1, the output of the last neural unit is {output_neural_1} and the loss is {loss_1}.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These tests allow us to check that the loss is higher when the prediction of the network is wrong : the forward functions seem to be programmed properly."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 3: Backprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
