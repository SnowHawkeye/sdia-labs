{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep learning - Practical nÂ°2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Student 1 : Emilie SALEM\n",
        "## Student 2 : Hadrien SALEM "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_circles\n",
        "import matplotlib.pyplot as plt\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generation and display of the dataset\n",
        "X,y = make_circles(n_samples = 300, noise=0.2, factor=0.1)\n",
        "n,d = X.shape\n",
        "plt.figure(1,figsize=[6,6])\n",
        "plt.plot(X[:,0][np.where(y==1)],X[:,1][np.where(y==1)],\".c\")\n",
        "plt.plot(X[:,0][np.where(y==0)],X[:,1][np.where(y==0)],\".m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions\n",
        "\n",
        "def sigmoid(x):\n",
        "    res = 1 / (1 + np.exp(-x))\n",
        "    return res\n",
        "\n",
        "def onehot(y,i):\n",
        "    \"\"\"Creates a one-hot encoding of the i element of y\"\"\"\n",
        "    return 1-np.array(list(bin(y[i]+1)[2:].zfill(2))).astype(int)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 1: Network creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definition of an input unit\n",
        "class InputUnit:\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,data):\n",
        "        self.data = data # one column of matrix X\n",
        "        self.n = data.shape[0] # dataset size\n",
        "        self.k = 0 # layer number\n",
        "        self.z = 0 # unit output\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Input unit in layer {self.k}, containing {self.n} examples.\"\n",
        "\n",
        "    def plug(self, unit):\n",
        "        unit.preceding.append(self)\n",
        "\n",
        "    def forward(self,i):\n",
        "        self.z = self.data[i]\n",
        "        return self.z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creation of 2 input units, one for each column of X\n",
        "input_unit_1 = InputUnit(X[:,0])\n",
        "input_unit_2 = InputUnit(X[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definition of a neural unit\n",
        "class NeuralUnit:\n",
        "    \n",
        "    # Constructor\n",
        "    def __init__(self,k,u):\n",
        "        self.u = u # unit number\n",
        "        self.preceding = [] # list of preceding neurons\n",
        "        # self.npr = 0 # length of list preceding\n",
        "        self.following = [] # list of following neurons\n",
        "        # self.nfo = 0 # length of list following\n",
        "        self.k = k # layer number\n",
        "        self.w = 0 # unit weights\n",
        "        self.b = 0 # unit intercept\n",
        "        self.z = 0 # unit output\n",
        "        self.delta = np.zeros(np.shape(self.w))\n",
        "        self.w_grad = np.zeros(np.shape(self.w))\n",
        "        self.b_grad = 0\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Neural unit #{self.u} in layer {self.k} with {self.npr} preceding element(s) and {self.nfo} following element(s).\"\n",
        "\n",
        "    @property # custom getter for length of list preceding\n",
        "    def npr(self):\n",
        "        return len(self.preceding)\n",
        "\n",
        "    @property # custom getter for length of list following\n",
        "    def nfo(self):\n",
        "        return len(self.following)\n",
        "\n",
        "    def reset_params(self):\n",
        "        self.w = np.random.randn(self.npr)\n",
        "        self.b = np.random.randn()\n",
        "\n",
        "    def plug(self, unit):\n",
        "        unit.preceding.append(self)\n",
        "        self.following.append(unit)\n",
        "\n",
        "    def forward(self, i):\n",
        "        z_in = np.zeros(self.npr)\n",
        "        for index, unit in enumerate(self.preceding) :\n",
        "            z_in[index] = unit.forward(i)\n",
        "        self.z = sigmoid(self.w.dot(z_in) + self.b)\n",
        "        return self.z\n",
        "\n",
        "    def backprop(self, i, deltas):\n",
        "        self.delta = np.zeros(np.shape(self.w))\n",
        "        self.w_grad = np.zeros(np.shape(self.w))\n",
        "        delta_u = deltas[self.u]\n",
        "        for v in range(self.npr):\n",
        "            self.delta[v] = (self.z*(1 - self.z)*self.w[v])*delta_u\n",
        "            self.w_grad[v] = self.preceding[v].z*self.z*(1-self.z)*delta_u\n",
        "        self.b_grad = self.z*(1-self.z)*delta_u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creation of the 3 neural units corresponding to this problem\n",
        "\n",
        "# Layer 1\n",
        "neural_unit_11 = NeuralUnit(1, 1)\n",
        "neural_unit_12 = NeuralUnit(1, 2)\n",
        "\n",
        "# Layer 2\n",
        "neural_unit_21 = NeuralUnit(2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Loss:\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,y,k):\n",
        "        self.preceding = [] # list of preceding neurons\n",
        "        # self.npr = 0 # length of list preceding\n",
        "        self.y = y # array of class labels of the training data\n",
        "        self.k = k # layer index\n",
        "        self.delta = np.zeros((1,))\n",
        "            \n",
        "    def __str__(self):\n",
        "        return f\"Loss unit in layer {self.k} with {self.npr} preceding element(s).\"\n",
        "\n",
        "    @property # custom getter for length of list preceding\n",
        "    def npr(self):\n",
        "        return len(self.preceding)\n",
        "\n",
        "    def forward(self, i):\n",
        "        z_in = self.preceding[0].forward(i)\n",
        "        if self.y[i] == 0 : return -np.log(1-z_in)\n",
        "        else : return -np.log(z_in)\n",
        "            \n",
        "    def backprop(self, i):\n",
        "        z_in = self.preceding[0].z\n",
        "        if self.y[i] == 0 : self.delta[0] = 1/(1-z_in)\n",
        "        else : self.delta[0] = -1/z_in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creation of the loss unit\n",
        "loss_unit = Loss(y, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    # Constructor\n",
        "    def __init__(self,X,y,archi):\n",
        "        self.archi = archi\n",
        "        self.X = X\n",
        "        self.n = X.shape[0]\n",
        "        self.y = y\n",
        "        self.K = len(archi) # number of layers (including the input layer but without the loss layer)\n",
        "\n",
        "        # Creating network\n",
        "        net = []\n",
        "\n",
        "        # Inputs\n",
        "        n_inputs = archi[0]\n",
        "        input_layer = []\n",
        "        for i in range(n_inputs):\n",
        "            input_layer.append(InputUnit(X[:,i]))\n",
        "        net.append(input_layer)\n",
        "            \n",
        "        # Neural units\n",
        "        for k in range(1, self.K) :\n",
        "            layer = []\n",
        "            n_units = archi[k]\n",
        "            for j in range(n_units):\n",
        "                layer.append(NeuralUnit(k,j))\n",
        "            net.append(layer)\n",
        "\n",
        "        # Loss\n",
        "        loss_layer = [Loss(y, self.K)]\n",
        "        net.append(loss_layer)\n",
        "\n",
        "        # Plugging units together and resetting parameters\n",
        "        for k in range(len(net) - 1):\n",
        "            for unit in net[k] :\n",
        "                if isinstance(unit, NeuralUnit) : unit.reset_params()\n",
        "                for next_unit in net[k+1]:\n",
        "                    unit.plug(next_unit)\n",
        "\n",
        "        self.net = net\n",
        "\n",
        "    def forward(self,i):\n",
        "        return self.net[-1][0].forward(i)\n",
        "\n",
        "    def backprop(self, i):\n",
        "        # first iteration: loss layer\n",
        "        self.net[-1][0].backprop(i)\n",
        "        deltas = self.net[-1][0].delta\n",
        "\n",
        "        for k in range(len(self.net)-2, 0, -1):\n",
        "            deltas_new = np.zeros((self.net[k][0].npr,))\n",
        "            for u in range(len(self.net[k])):\n",
        "                self.net[k][u].backprop(i,deltas)\n",
        "                deltas_new += self.net[k][u].delta\n",
        "            deltas = deltas_new\n",
        "\n",
        "    def update(self, eta):\n",
        "        for k in range(1, len(self.net) - 1):\n",
        "            for unit in self.net[k]:\n",
        "                unit.w -= eta * unit.w_grad\n",
        "                unit.b -= eta * unit.b_grad\n",
        "    \n",
        "    def train(self, epochs,eta):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(self.n):\n",
        "                self.forward(i)\n",
        "                self.backprop(i)\n",
        "                self.update(eta)\n",
        "\n",
        "    def predict(self, i):\n",
        "        return self.net[-2][0].forward(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instanciation of the network corresponding to our problem\n",
        "archi = [2,2,1]\n",
        "mlp = MLP(X,y,archi)\n",
        "\n",
        "# Check unit connections and parameters initialization\n",
        "for layer in mlp.net :\n",
        "    for unit in layer :\n",
        "        print(unit)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 2: Forward pass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We programmed the `forward(self,i)` functions in all the above classes (different types of units and `MLP`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checking that the forward methods are correctly programmed\n",
        "\n",
        "index_0 = 0 # index of an element for which y = 0\n",
        "loss_0 = mlp.forward(index_0)\n",
        "output_neural_0 = mlp.net[2][0].z # output of the last neural unit of the network\n",
        "print(f\"For an example of class 0, the output of the last neural unit is {output_neural_0} and the loss is {loss_0}.\")\n",
        "\n",
        "\n",
        "index_1 = 45 # index of an element for which y = 1\n",
        "loss_1 = mlp.forward(index_1)\n",
        "output_neural_1 = mlp.net[2][0].z\n",
        "print(f\"For an example of class 1, the output of the last neural unit is {output_neural_1} and the loss is {loss_1}.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These tests allow us to check that the loss is higher when the prediction of the network is wrong : the forward functions seem to be programmed properly."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 3: Backprop"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We added `backprop` methods to the `Loss` and `NeuralUnit` classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing the computed gradients\n",
        "i = 0\n",
        "archi = [d,10,1]\n",
        "mlp = MLP(X,y,archi)\n",
        "mlp.forward(i)\n",
        "mlp.backprop(i)\n",
        "epsi=1e-3\n",
        "mlp2 = copy.deepcopy(mlp)\n",
        "mlp2.net[1][0].w[0] = mlp.net[1][0].w[0] + epsi\n",
        "print(\"numerical derivative is:\",(mlp2.forward(i) - mlp.forward(i))/epsi)\n",
        "print(\"computed derivative is:\",mlp.net[1][0].w_grad[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The numerical derivatives are very close to the computed ones: it seems that our implementation is correct."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 4 : SGD training of the network"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We implemented an `update` and a `train` method in the `MLP` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training the mlp\n",
        "mlp.train(50,0.01)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also implemented the `predict` method in the `MLP` class, which we use in the test below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h = .02\n",
        "x1_min, x1_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "x2_min, x2_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "x11, x22 = np.meshgrid(np.arange(x1_min, x1_max, h),np.arange(x2_min, x2_max, h))\n",
        "X_disp = np.c_[x11.ravel(), x22.ravel()]\n",
        "n_disp = X_disp.shape[0]\n",
        "Z = []\n",
        "for u in range(mlp.archi[0]):\n",
        "    mlp.net[0][u].data = X_disp[:,u]\n",
        "for i in range(n_disp):\n",
        "    Z.append(mlp.predict(i))\n",
        "for u in range(mlp.archi[0]):\n",
        "    mlp.net[0][u].data = X[:,u]\n",
        "Z = np.array(Z)\n",
        "Z = Z.reshape(x11.shape)\n",
        "plt.figure(2,figsize=[6,6])\n",
        "plt.plot(X[:,0][np.where(y==1)],X[:,1][np.where(y==1)],'.r')\n",
        "plt.plot(X[:,0][np.where(y==0)],X[:,1][np.where(y==0)],'.b')\n",
        "plt.contourf(x11, x22, Z, cmap=plt.cm.bwr, alpha=.8)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although it varies with weight initialization, we observe that the prediction often seems to be fairly accurate: it successfully distinguished two concentric clusters. A more in-depth study of the results would be necessary to tell how accurate the decision boundary is.\n",
        "\n",
        "However, for some weight initializations, we can see that the algorithm does not always converge."
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "753521603511815aa6ece17250c3d0eeb00bf2a0c2940239f15002b3e6c9db03"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('sdia-deep': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
