{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TP3 : Logistic regression\n",
        "\n",
        "The purpose of this tutorial is to implement and use the Logistic Regression for binary classification. We will apply this\n",
        "method to the problem of handwritten characters to learn how to\n",
        "distinguish two numbers (here 5 and 6)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pylab import *\n",
        "#import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "from numpy import linalg as la \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import sklearn as skl\n",
        "import seaborn as sns"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Logistic regression, IRLS algorithm."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preliminary question: the algorithm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Have a look at the function `regression_logistique.m` and locate the main steps of the algorithm you have been taught (see course).\n",
        "You can comment the code in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def regression_logistique(X,t,Nitermax=20,eps_conv=1e-3):\n",
        "    '''Entrees :\n",
        "    X = [ones(N_train,1) x_train];\n",
        "    t = class_train \n",
        "    Nitermax = nombre maximale d'itérations (20 par défaut)\n",
        "    eps_conv = critère de convergence sur norm(w-w_old)/norm(w) ; \n",
        "    eps_conv=1e-3 par défaut\n",
        "    \n",
        "    Sorties : \n",
        "    w : vecteur des coefficients de régression logistique\n",
        "    Niter : nombre d'itérations utilisées effectivement\n",
        "   \n",
        "   Fonction de régression logistique pour la classification binaire.\n",
        "   \n",
        "   Utilisation :\n",
        "       Nitermax = 50\n",
        "       eps_conv = 1e-4\n",
        "       [w,Niter] = regression_logistique(X,t,Nitermax,eps_conv)\n",
        "    '''\n",
        "    N_train = X.shape[0]\n",
        "\n",
        "    #initialisation : 1 pas de l'algorithme IRLS\n",
        "    w = np.zeros((X.shape[1],)) # w initialisé comme vecteur nul de dimension le nombre d'attributs\n",
        "    w_old = w \n",
        "    y = 1/2*np.ones((N_train,)) # on initialise l'activation (\"w.x\") à 0, y = sgm(wx) vaut alors 0.5\n",
        "    R = np.diag(y*(1-y))   # diag(y_n(1-y_n))\n",
        "    z = X.dot(w_old)-la.inv(R).dot(y-t) # initialisation de z (old)\n",
        "    w = la.inv(X.T.dot(R).dot(X)).dot(X.T).dot(R).dot(z) # calcul de w pour la première étape avec les valeurs initialisées\n",
        "\n",
        "    # boucle appliquant l'algorithme de Newton-Raphson\n",
        "    Niter = 1 # on a déjà fait \"l'itération 1\"\n",
        "    while  (la.norm(w-w_old)/la.norm(w)>eps_conv) & (Niter<Nitermax) : # condition d'arrêt: soit w et w_old sont suffisamment proches, soit on atteint un nombre maximum d'itérations\n",
        "        Niter = Niter+1\n",
        "        y = 1/(1+np.exp(-X.dot(w))) # on calcule y avec la fonction sigma\n",
        "        R = np.diag(y*(1-y)) # par définition de R\n",
        "        # on calcule ensuite z et w avec les formules données par la méthode de Newton-Raphson\n",
        "        w_old = w \n",
        "        z = X.dot(w_old)-la.inv(R).dot(y-t)\n",
        "        w = la.inv(X.T.dot(R).dot(X)).dot(X.T).dot(R).dot(z)\n",
        "         \n",
        "    return w, Niter"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading & preparing synthetic data\n",
        "\n",
        "Load the training and test data sets `synth_train.txt`\n",
        "and `synth_test.txt`. The targets t belong to {1,2} and the features  \n",
        "x belong to R^2. \n",
        "\n",
        "We have 100 training samples and 200 test samples\n",
        "\n",
        "* the 1st column contains the label of each sample, \n",
        "* columns 2 and 3 contain the coordinate of each point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training set\n",
        "synth_train = np.loadtxt('synth_train.txt') \n",
        "class_train = synth_train[:,0]\n",
        "class_train_1 = np.where(synth_train[:,0]==1)[0]\n",
        "class_train_2 = np.where(synth_train[:,0]==2)[0]\n",
        "x_train = synth_train[:,1:]\n",
        "N_train = np.size(x_train,axis=0)\n",
        "\n",
        "# Test set\n",
        "synth_test = np.loadtxt('synth_test.txt')\n",
        "class_test = synth_test[:,0]\n",
        "class_test_1 = np.where(synth_test[:,0]==1)[0]\n",
        "class_test_2 = np.where(synth_test[:,0]==2)[0]\n",
        "x_test = synth_test[:,1:]\n",
        "N_test = np.size(x_test,axis=0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing features for logistic regression (binary classification)\n",
        "First, we prepare the feature matrix and the target vector associated to \n",
        "the training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = np.hstack((np.ones((N_train,1)),x_train))\n",
        "t = 2-class_train   # 0 if class=2, 1 if class=1\n",
        "\n",
        "X_test = np.hstack((np.ones((N_test,1)),x_test))\n",
        "t_test = 2-class_test   # 0 if class=2, 1 if class=1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1 : the logistic function of decision\n",
        "\n",
        "1. Use the function `regression_logistique.m` to estimate the logistic\n",
        "regression vector `w`. *Indication : use `Nitermax = 50;\n",
        "eps_conv=1e-3;`.*\n",
        "2. Compute the decision function $f(x) = argmax_k P(C_k|x)$ on the test set\n",
        "to get the classification results. Recall that $y_n=\\sigma(w^T x)$ (logistic function)\n",
        "and that *using vectors* you may directly write $y=\\sigma(Xw)$, with the\n",
        "column of ones in X.\n",
        "3. Display the results by plotting the points from both the training set\n",
        "and the test set.\n",
        "4. Write the equation which defines the decision boundary.\n",
        "5. Artificially add a few points to the training set far from the decision boundary to check the robustness of logistic regression to outliers. Check the behaviour of LDA for comparison in this case and comment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1: Estimate the logistic regression vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w, Niter = regression_logistique(X, t, Nitermax=50)\n",
        "print(f\"The regression vector is {w} and was found in {Niter} iterations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2: Compute the decision function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigma(a): return 1/(1+np.exp(-a)) # sigmoid function\n",
        "\n",
        "class LogisticRegression():\n",
        "    \n",
        "    def __init__(self, train_data, train_target, threshold = 0.5, Nitermax = 50, eps_conv = 1e-3):\n",
        "        self.train_data = train_data\n",
        "        self.train_target = train_target\n",
        "        self.threshold = threshold\n",
        "        self.Nitermax = Nitermax\n",
        "        self.eps_conv = eps_conv\n",
        "        self.coefs, self.Niter = self.__get_coeffs()\n",
        "        \n",
        "    def __get_coeffs(self):\n",
        "        return regression_logistique(self.train_data, self.train_target, self.Nitermax, self.eps_conv)\n",
        "        \n",
        "    def predict(self, test_data):\n",
        "        pred = sigma(test_data.dot(self.coefs))\n",
        "        return np.where(pred >= self.threshold, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logReg = LogisticRegression(X, t)\n",
        "logReg.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3: Display results\n",
        "\n",
        "With the goal to use cross-validation and compare with LDA later on, we create a utility class that will help us display data and predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BinaryPointsClassification():\n",
        "    \"\"\"Utility class to help with displaying prediction results and error rates.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_data, train_target, test_data, test_target, f_predict):\n",
        "    \n",
        "        self.train_data = train_data\n",
        "        self.train_target = train_target\n",
        "        self.test_data = test_data\n",
        "        self.test_target = test_target\n",
        "        self.f_predict = f_predict\n",
        "        \n",
        "        self.prediction = self.f_predict(test_data)\n",
        "        self.errors, self.error_rate = self.__errors()\n",
        "        \n",
        "        self.confusion_matrix = skl.metrics.confusion_matrix(self.test_target, self.prediction)\n",
        "        \n",
        "        \n",
        "    def __errors(self):\n",
        "        \"\"\"Returns the points that were classified wrongly and the error rate.\n",
        "        \"\"\"\n",
        "        \n",
        "        is_wrong_prediction = np.logical_or(np.logical_and(self.prediction[:] == 1, self.test_target[:] == 0), np.logical_and(self.prediction[:] == 0, self.test_target[:] == 1))\n",
        "        errors = self.test_data[is_wrong_prediction, :][:,1:]\n",
        "        error_rate = len(errors) / len(self.test_data)\n",
        "        \n",
        "        return errors, error_rate\n",
        "    \n",
        "    \n",
        "    def predicted_points_figure(self, title=\"\"):\n",
        "        \"\"\"Given a set of points and binary predictions (0-1), returns a graph showing the classification, crossing out the mistakes.\n",
        "        \"\"\"\n",
        "        \n",
        "        x_predicted_0 = self.test_data[self.prediction[:] == 0, :][:,1:]\n",
        "        x_predicted_1 = self.test_data[self.prediction[:] == 1, :][:,1:]\n",
        "\n",
        "        plt.scatter(x_predicted_0[:,0], x_predicted_0[:,1], c='r')\n",
        "        plt.scatter(x_predicted_1[:,0], x_predicted_1[:,1], c='g')\n",
        "        plt.scatter(self.errors[:,0], self.errors[:,1], c='b', marker = 'x')\n",
        "        plt.title(title + \"\\n(mistakes are crossed out in blue)\")\n",
        "        \n",
        "        \n",
        "    def decision_regions_figure(self, x1_bounds, x2_bounds, resolution=1000):\n",
        "        \"\"\"Displays the decision regions for a given resolution (number of points along each axis).\n",
        "        \"\"\"\n",
        "\n",
        "        x1 = np.linspace(x1_bounds[0], x1_bounds[1], resolution) \n",
        "        x2 = np.linspace(x2_bounds[0], x2_bounds[1], resolution) \n",
        "        X1, X2 = meshgrid(x1,x2)\n",
        "        \n",
        "        X = np.stack((np.ones(resolution**2), np.ravel(X1), np.ravel(X2))).T\n",
        "        Z = self.f_predict(X).reshape(X1.shape)\n",
        "\n",
        "        plt.contourf(X1, X2, Z, colors=['r', 'g', 'b', \"y\", 'purple'], alpha = 0.3) # extra colors are necessary otherwise everything is green and the boundary is red         \n",
        "        \n",
        "        \n",
        "    def confusion_matrix_heatmap(self):\n",
        "        \"\"\"Displays the heatmap corresponding to the confusion matrix.\"\"\"\n",
        "        sns.heatmap(self.confusion_matrix, annot=True, fmt='d')      \n",
        "         \n",
        "        \n",
        "    def display_prediction(self, x1_bounds, x2_bounds, resolution = 1000, figsize=(20,5) , title = \"Classification\"):\n",
        "        \"\"\"Convenient all-in-one display function. Displays the decision regions, the classified points (with mistakes) and the confusion matrix heatmap.\n",
        "        \"\"\"\n",
        "        \n",
        "        plt.figure(figsize=figsize)\n",
        "\n",
        "        plt.subplot(121)\n",
        "        self.predicted_points_figure(title)\n",
        "        self.decision_regions_figure(x1_bounds, x2_bounds, resolution)\n",
        "\n",
        "        plt.subplot(122)\n",
        "        plt.title(\"Confusion matrix\")\n",
        "        self.confusion_matrix_heatmap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x1_bounds=(-2.5, 1.5)\n",
        "x2_bounds=(-1, 5)\n",
        "figsize=(20,5)\n",
        "\n",
        "logReg = LogisticRegression(X, t)\n",
        "\n",
        "# Results for the test set\n",
        "classification_test = BinaryPointsClassification(X, t, X_test, t_test, logReg.predict)\n",
        "classification_test.display_prediction(x1_bounds, x2_bounds, resolution = 1000, title = f\"Test set classification (error rate: {classification_test.error_rate*100}%)\")\n",
        "\n",
        "# Results for the training set\n",
        "classification_train = BinaryPointsClassification(X, t, X, t, logReg.predict)\n",
        "classification_train.display_prediction(x1_bounds, x2_bounds, resolution = 1000, figsize = figsize, title = f\"Train set classification (error rate: {classification_train.error_rate*100}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q4: Equation of the decision boundary\n",
        "\n",
        "The decision boundary corresponds to the $(x1,x2)$ couples such that $\\sigma(w^T x) = 0$. In other words:  $w_0 + w_1 x_1 + w_2 x_2 = 0 $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5: Add a few points away from the decision boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_points = 10\n",
        "outliers = np.stack([np.ones(nb_points), np.linspace(3,4,nb_points), np.linspace(-1,0,nb_points)]).T # generate 10 points far from the decision boundary\n",
        "\n",
        "X_outliers = np.vstack([X, outliers])\n",
        "t_outliers = np.hstack([t, np.zeros(nb_points)]) # all added points are in class 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training the LDA model\n",
        "\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "lda.fit(X_outliers, t_outliers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Displaying results\n",
        "\n",
        "x1_bounds = (-2.5, 4.5)\n",
        "x2_bounds=(-1, 5)\n",
        "\n",
        "classification_outliers = BinaryPointsClassification(X_outliers, t_outliers, X_test, t_test, logReg.predict)\n",
        "classification_outliers_lda = BinaryPointsClassification(X_outliers, t_outliers, X_test, t_test, lda.predict)\n",
        "\n",
        "plt.figure(figsize=(30,5))\n",
        "\n",
        "plt.subplot(141)\n",
        "plt.scatter(outliers[:,1], outliers[:,2], c='orange')\n",
        "classification_outliers_lda.decision_regions_figure(x1_bounds, x2_bounds)\n",
        "classification_outliers_lda.predicted_points_figure(title=f\"LDA (error rate: {classification_outliers_lda.error_rate*100}%)\")\n",
        "\n",
        "plt.subplot(142)\n",
        "plt.scatter(outliers[:,1], outliers[:,2], c='orange')\n",
        "classification_outliers.decision_regions_figure(x1_bounds, x2_bounds)\n",
        "classification_outliers.predicted_points_figure(title=f\"Logistic regression (error rate: {classification_outliers.error_rate*100}%)\")\n",
        "\n",
        "plt.subplot(143)\n",
        "plt.title(\"Confusion matrix for LDA\")\n",
        "classification_outliers_lda.confusion_matrix_heatmap()\n",
        "\n",
        "plt.subplot(144)\n",
        "plt.title(\"Confusion matrix for Logistic regression\")\n",
        "classification_outliers.confusion_matrix_heatmap()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# --- TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO ---\n",
        "\n",
        "### Extra: Cross-validation\n",
        "\n",
        "In order to better estimate the error rate and find an optimal threshold, we realize a cross validation. We will test 10 values between 0.4 and 0.6.\n",
        "\n",
        "# --- TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO ---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Application: handwritten digits recognition 5 & 6\n",
        "We load 2 matrices which contain each a sequence of examples of 16x16 images \n",
        "of handwritten digits which are 5 and 6 here. Each line of the matrix\n",
        "contains 256 pixel values coding for the gray level of a 16x16 image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_5 = np.loadtxt('train_5.txt',delimiter=',')   # 556 samples\n",
        "train_6 = np.loadtxt('train_6.txt',delimiter=',')   # 664 samples"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Examples of images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Digit 5\n",
        "n=9\n",
        "I = np.reshape(train_5[n,:],(16,16))\n",
        "\n",
        "plt.imshow(I,cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Digit 6\n",
        "n=5;\n",
        "I = reshape(train_6[n,:],(16,16))\n",
        "\n",
        "plt.imshow(I,cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Separating the training and test sets\n",
        "\n",
        "We keep in the training set the 145 first images of 5s and the 200 first\n",
        "images of 6s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_brut = np.vstack((train_5[:145,:], train_6[:200,:]))\n",
        "N_train = np.size(x_train_brut,axis=0)\n",
        "class_train = np.ones((345,)) # label 1 for digit 6\n",
        "class_train[:145] = 0 # label 0 for digit 5\n",
        "\n",
        "x_test_brut = np.vstack((train_5[145:,:], train_6[200:,:]))\n",
        "N_test = np.size(train_5,axis=0)+np.size(train_6,axis=0)-N_train\n",
        "class_test = np.ones((875,))\n",
        "class_test[412:] = 0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: logistic regression to classify 5 & 6\n",
        "\n",
        "1. Note that pixel values are between -1 and 1 by using the functions\n",
        " `min(I(:))` and `max(I(:))`.\n",
        "2. Identify the indices of the most significant pixels, which are defined \n",
        "as having a standard deviation greater than 0.5 here. We denote by `lis_sig`\n",
        "the list of positions of these significant pixels in the image vector.\n",
        "_Indication : the function `std` gives the standard deviation (columnwise\n",
        "in matrices) and you should find 173 pixel positions.\n",
        "3. Show a binary image to locate these pixels.\n",
        "_Indication : `Isig = zeros(16); Isig(list_sig)=1; Isig=Isig';`._\n",
        "4. Define the training set `x_train` from `x_train_brut` from the significant pixels only.\n",
        "5. Do the same with `x_test_brut` to extract `x_test`.\n",
        "6. Use `regression_logistique.m` to estimate the logistic regression vector\n",
        "`w` from the training set `x_train`. \n",
        "Choose `Nitermax = 13; eps_conv = 1e-3;`\n",
        "7. Compute the decision function and the labels of the test set `x_test`. \n",
        "_Indication : do not forget the column of ones !_\n",
        "8. Estimate the classification error rate by using :\n",
        "`erreur = sum(abs(class-class_test))/N_test;`.\n",
        "9. Locate some misclassified examples and visualize the corresponding image.\n",
        "Comment on your results and observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1: Minimum and maximum pixel values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"The pixel values are between {np.min(I[:])} and {np.max(I[:])}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2: Identify the most significant pixels\n",
        "and\n",
        "### Q3: Display a binary image to locate these pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "std = np.std(x_train_brut, axis=0)\n",
        "\n",
        "I_std_heat = std.reshape((16,16))\n",
        "\n",
        "std_bin = np.array([std>0.5])[0]\n",
        "I_std_bin = std_bin.reshape((16,16))\n",
        "\n",
        "plt.figure(figsize=(13,4.6))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Standard deviation for each pixel over the entire dataset\")\n",
        "sns.heatmap(I_std_heat)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(f\"Display of the {sum(std_bin)} most important pixels (std>0.5)\")\n",
        "sns.heatmap(I_std_bin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pixels that have the highest standard deviation are located in such a way that we could fit 5's and 6's in the \"silhouette\" that is produced from the heatmap. Understandably, we recognize the round shape that is common to 5 and 6 on the bottom right of the image, among other common features. This translates well the fact that those pixels are the ones that are important to look at when it comes to differentiating the two numbers.\n",
        "\n",
        "Out of curiosity, we also display the most significant pixels for 5's and 6's individually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "std_5 = np.std(train_5, axis=0)\n",
        "std_6 = np.std(train_6, axis=0)\n",
        "\n",
        "plt.figure(figsize=(13,4.6))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Standard deviation for each pixel in the 5's dataset\")\n",
        "sns.heatmap(std_5.reshape((16,16)))\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(\"Standard deviation for each pixel in the 6's dataset\")\n",
        "sns.heatmap(std_6.reshape((16,16)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q4 and Q5: Define the training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = x_train_brut[:, std_bin]\n",
        "x_test = x_test_brut[:, std_bin]\n",
        "\n",
        "print(f\"The number of columns in x_train was reduced to {x_train.shape[1]}\")\n",
        "print(f\"The number of columns in x_test was reduced to {x_test.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6: Estimate the logistic regression vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Nitermax = 13\n",
        "eps_conv = 1e-3\n",
        "\n",
        "x_train_t = np.hstack((np.ones((N_train,1)),x_train))\n",
        "x_test_t = np.hstack((np.ones((N_test,1)),x_test))\n",
        "\n",
        "logRegNumbers = LogisticRegression(x_train_t, class_train, Nitermax = Nitermax, eps_conv = eps_conv)\n",
        "\n",
        "# The logistic regression vector is given by\n",
        "w = logRegNumbers.coefs\n",
        "\n",
        "print(f\"The logistic regression vector is of shape {w.shape} (includes 173 most significant pixels and bias)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7: Compute the prediction\n",
        "\n",
        "and \n",
        "\n",
        "### Q8: Estimate the error rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction = logRegNumbers.predict(x_test_t)\n",
        "prediction = 1 - prediction # this prediction seems to give the opposite labels, so we flip 1's and 0's\n",
        "\n",
        "error = sum(abs(prediction-class_test))/N_test\n",
        "print(f\"The estimated error rate for the numbers prediction is {error*100}%.\")\n",
        "\n",
        "confusion = skl.metrics.confusion_matrix(class_test, prediction)\n",
        "sns.heatmap(confusion, annot=True, fmt='d')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9: Display and comment on some of the mistakes\n",
        "\n",
        "\n",
        "TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Logistic regression using `scikit-learn`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Go to** http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html for a presentation of the logistic regression model in `scikit-learn`.\n",
        "\n",
        "2. **Apply** it to the present data set.\n",
        "\n",
        "3. **Comment** on the use of logistic regression.\n",
        "\n",
        "*Indication : you may have a look at* \n",
        "\n",
        "a) Theory : http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex5/ex5.html\n",
        "\n",
        "b) Video :  https://www.coursera.org/learn/machine-learning/lecture/4BHEy/regularized-logistic-regression \n",
        "\n",
        "c) Example : http://scikit-learn.org/stable/auto_examples/exercises/plot_digits_classification_exercise.html#sphx-glr-auto-examples-exercises-plot-digits-classification-exercise-py\n",
        "\n",
        "*for a short presentation of regularized logistic regression.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Include your code here\n",
        "from sklearn.linear_model import LogisticRegression as SkLearnLogisticRegression\n",
        "# ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Commentaires :"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "eddbe03df7610b6ab8092acf41ddda77de6230c42b3d7be1a47827e0e54e49a3"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('sdia-decision': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
