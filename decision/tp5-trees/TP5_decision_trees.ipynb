{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP5: Decision trees & random forests\n",
    "The aim of this tutorial is to get familiar with the use of decision trees and their generalizations on simple examples using `scikit-learn` tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing your installation first\n",
    "You will need to install packages `python-graphviz` first. If needed, uncomment the `conda` command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If needed, uncomment the line below:\n",
    "# pip install graphviz\n",
    "# import os\n",
    "# os.environ[\"PATH\"] += os.pathsep + 'C:\\\\Program Files\\\\Graphviz\\\\bin\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "\n",
    "# Load matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the library with the iris dataset\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "\n",
    "# Load scikit's decision tree classifier\n",
    "from sklearn import tree\n",
    "\n",
    "# Load scikit's random forest classifier library\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# To visualize trees\n",
    "import graphviz \n",
    "\n",
    "# Load pandas to manipulate data frames (Excel like)\n",
    "import pandas as pd\n",
    "\n",
    "# Load seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this tutorial is famous. Called, **the iris dataset**, it contains four variables measuring various parts of iris flowers of three related species, and then a fourth variable with the species name. The reason it is so famous in machine learning and statistics communities is because the data requires very little preprocessing (i.e. no missing values, all features are floating numbers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: explore the data set\n",
    "1. What is the structure of the object `iris` ?\n",
    "\n",
    "2. Plot this dataset in a well chosen set of representations to explore the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `pandas` to manipulate the data\n",
    "Pandas is great to manipulate data in a Microsoft Excel like way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe with the four feature variables\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# View the top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column with the species names, this is what we are going to try to predict\n",
    "df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
    "\n",
    "# View the top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "`iris` is a `sklearn.utils.Bunch` object, which is an extension of a dictionary. It contains the following key/value couples:\n",
    "- `data`: a `numpy.array` of `float` containing the different attributes for each sample of the Iris dataset.\n",
    "- `feature_names`: a `list` explaining what each value of a sample corresponds to. Here, they correspond to various characteristics of the flower, given in cm.\n",
    "- `target`: a `numpy.array` of `int` containing the class label of each sample.\n",
    "- `target_names`: a `numpy.array` of `String` explaining what species of flower each class label corresponds to.\n",
    "- `filename`: a `string` containing the name of the file the data is from.\n",
    "- `data_module`: a `string` containing the `sklearn` module the data is from.\n",
    "\n",
    "There are key/value couples containing metadata about the dataset that we won't be using here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 150 samples, equally distributed between the 3 species (50 of each).\n",
    "\n",
    "Let's represent this dataset using a `seaborn.pairplot`, which plots pairwise relationships between the features. It allows us to have a visual representation of these attributes even though the dimension of data is greater than 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that in this pairwise representation, the `setosa` species seems to always be linearly separable from the other two species. However, there is more overlap between the `versicolor` and `virginica` species, which could make the prediction more challenging for these two classes.\n",
    "\n",
    "We can also observe that the separation between the 3 classes is clearer for the `petal width` and `petal length` attributes, which means they will probably be decisive attributes for the classification.\n",
    "\n",
    "Let's use a last representation to confirm our observations: a `boxplot`, which shows the repartition of data for a single attribute. Let's compare the `boxplots` of one attribute for which the distribution between classes is clear (`petal length`), and one for which it isn't (`sepal length`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(121)\n",
    "sns.boxplot(x=\"species\", y=\"petal length (cm)\", data=df)\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.boxplot(x=\"species\", y=\"sepal length (cm)\", data=df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the distribution for `petal length` is very concentrated, even though there is a bit of overlap between `versicolor` and `virginica`. For `sepal length`, the distributions are more spread which causes overlap between the 3 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: create training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column that for each row, generates a random number between 0 and 1, and if that value is less than or equal to .75, then sets the value of that cell as True and false otherwise. This is a quick and dirty way of randomly assigning some rows to be used as the training data and some as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n",
    "\n",
    "# View the top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new dataframes, one with the training rows, one with the test rows\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the number of observations for the test and training dataframes\n",
    "print('Number of observations in the training data:', len(train))\n",
    "print('Number of observations in the test data:',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the feature column's names\n",
    "features = df.columns[:4]\n",
    "\n",
    "# View features\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['species'] contains the actual species names. Before we can use it,\n",
    "# we need to convert each species name into a digit. So, in this case there\n",
    "# are three species, which have been coded as 0, 1, or 2.\n",
    "y = pd.factorize(train['species'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: decision trees for the iris dataset\n",
    "The method `tree.DecisionTreeClassifier()` from `scikit-learn` builds decision trees objects as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(train[features], y)\n",
    "\n",
    "# Using the whole dataset you may use directly:\n",
    "#clf = clf.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `export_graphviz` exporter supports a variety of aesthetic options, including coloring nodes by their class (or value for regression) and using explicit variable and class names if desired. Jupyter notebooks also render these plots inline automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also export the tree in Graphviz format and  savethe resulting graph in an output file iris.pdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After being fitted, **the model can then be used to predict the class of samples**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred = clf.predict(iris.data[:1, :])\n",
    "class_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "1. Train the decision tree on the iris dataset and explain how one should read blocks in `graphviz` representation of the tree.\n",
    "\n",
    "2. Plot the regions of decision with the points of the training set superimposed.\n",
    "\n",
    "*Indication: you may find the function `plt.contourf` useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "The decision tree was trained in the above cells. The blocks in `graphviz` give information about every node, including:\n",
    "- the attribute used to partition the node as well as the threshold used to classify samples;\n",
    "- the gini index, which is a measure of impurity: the closest it is to 0, the purest the node;\n",
    "- the number of samples in this part of the tree (reachable from this node);\n",
    "- the repartition of these samples in the 3 classes;\n",
    "- the dominant class in this node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to display pairwise regions of decision was found on the [scikit documentation website](https://scikit-learn.org/0.15/auto_examples/tree/plot_iris.html). We slightly changed the way predictions are displayed for clarity's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_classes = 3\n",
    "plot_colors = \"brg\"\n",
    "plot_step = 0.02\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n",
    "                                [1, 2], [1, 3], [2, 3]]):\n",
    "    # We only take the two corresponding features\n",
    "    X = iris.data[:, pair]\n",
    "    y = iris.target\n",
    "\n",
    "    # Shuffle\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.seed(13)\n",
    "    np.random.shuffle(idx)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "\n",
    "    # Standardize\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    X = (X - mean) / std\n",
    "\n",
    "    # Train\n",
    "    clf_display = tree.DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.subplot(2, 3, pairidx + 1)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = clf_display.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, colors=['b', 'y', 'g', 'r', 'purple'], alpha = 0.3)\n",
    "\n",
    "    plt.xlabel(iris.feature_names[pair[0]])\n",
    "    plt.ylabel(iris.feature_names[pair[1]])\n",
    "    plt.axis(\"tight\")\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c = color, label=iris.target_names[i],\n",
    "                    cmap=plt.cm.Pastel1)\n",
    "\n",
    "    plt.axis(\"tight\")\n",
    "\n",
    "plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, some pairs of attribute are more easily separable than others, especially those involving the `petal length` and `petal width` features. \n",
    "\n",
    "We also confirmed that `setosa` is more easily predicted than `versicolor` and `virginica`. However, even in the regions where there is a lot of overlap between them, the tree is able to separate them fairly well.\n",
    "\n",
    "We can also notice that there may be some overfitting, since some regions contain single isolated points of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "1. Build 2 different trees based on a sepal features (sepal lengths, sepal widths) vs petal features (petal lengths, petal widths) only: which features are the most discriminant?\n",
    "\n",
    "2. Compare performances with those obtained using all features.\n",
    "\n",
    "3. Try the same as above using the various splitting criterion available, Gini's index, classification error or cross-entropy. Comment on your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tree based on sepal features\n",
    "features_sepal = features[0:2]\n",
    "y = pd.factorize(train['species'])[0]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(train[features_sepal], y)\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names[0:2],  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tree based on petal features\n",
    "features_petal = features[2:4]\n",
    "y = pd.factorize(train['species'])[0]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(train[features_petal], y)\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names[2:4],  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could already tell from the regions of decision, the petal features seem to be much more discriminant than the sepal features. Indeed, the tree generated from sepal features is much more complex and deep than the one generated with petal features: it shows that each node struggles to distinguish classes based on sepal features only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the performances between the trees, we will compare the scores and confusion matrices they obtain on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "y = pd.factorize(train['species'])[0]\n",
    "y_test = pd.factorize(test['species'])[0]\n",
    "\n",
    "clf_full = tree.DecisionTreeClassifier()\n",
    "clf_full = clf_full.fit(train[features], y)\n",
    "\n",
    "clf_petal= tree.DecisionTreeClassifier()\n",
    "clf_petal = clf_petal.fit(train[features_petal], y)\n",
    "\n",
    "clf_sepal = tree.DecisionTreeClassifier()\n",
    "clf_sepal = clf_sepal.fit(train[features_sepal], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Scores\n",
    "pred_full = clf_full.predict(test[features])\n",
    "score_full = accuracy_score(y_test, pred_full)\n",
    "print(f\"The score obtained with all features is: {score_full}\")\n",
    "\n",
    "pred_petal = clf_petal.predict(test[features_petal])\n",
    "score_petal = accuracy_score(y_test, pred_petal)\n",
    "print(f\"The score obtained with petal features only is: {score_petal}\")\n",
    "\n",
    "pred_sepal = clf_sepal.predict(test[features_sepal])\n",
    "score_sepal= accuracy_score(y_test, pred_sepal)\n",
    "print(f\"The score obtained with sepal features only is: {score_sepal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "pd.crosstab(test['species'], pred_full, rownames=['Actual Species'], colnames=['Predicted Species (all features)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test['species'], pred_petal, rownames=['Actual Species'], colnames=['Predicted Species (petal features)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test['species'], pred_sepal, rownames=['Actual Species'], colnames=['Predicted Species (sepal features)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that:\n",
    "- sepal features give a very poor score, which was to be expected;\n",
    "- using petal features give an even better score than using all features: it shows that not only are sepal features not very discriminant, they even create more confusion for the classifier, which lowers the score;\n",
    "- the errors mostly occur with confusion between `versicolor` and `virginica`, further illustrating previous observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further ahead (not mandatory) \n",
    "Try the same approach adapted to another toy dataset from `scikit-learn` described at:\n",
    "http://scikit-learn.org/stable/datasets/index.html\n",
    "\n",
    "Play with another dataset available at:\n",
    "http://archive.ics.uci.edu/ml/datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Random forests\n",
    "Go to \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html \n",
    "\n",
    "for a documentation about the `RandomForestClassifier` provided by `scikit-learn`.\n",
    "\n",
    "Since target values must be integers, we first need to transform labels into numbers as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['species'] contains the actual species names. Before we can use it,\n",
    "# we need to convert each species name into a digit. So, in this case there\n",
    "# are three species, which have been coded as 0, 1, or 2.\n",
    "y = pd.factorize(train['species'])[0]\n",
    "\n",
    "# View target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest Classifier. By convention, clf means 'Classifier'\n",
    "rf = RandomForestClassifier(n_jobs=2, random_state=0)\n",
    "\n",
    "# Train the Classifier to take the training features and learn how they relate\n",
    "# to the training y (the species)\n",
    "rf.fit(train[features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions** and create actual english names for the plants for each predicted plant class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rf.predict(test[features])\n",
    "preds_names = pd.Categorical.from_codes(preds, iris.target_names)\n",
    "preds_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix unsing pandas:\n",
    "pd.crosstab(test['species'], preds, rownames=['Actual Species'], colnames=['Predicted Species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection using random forests byproducts\n",
    "\n",
    "One of the interesting use cases for random forest is feature selection. One of the byproducts of trying lots of decision tree variations is that you can examine which variables are working best/worst in each tree.\n",
    "\n",
    "When a certain tree uses one variable and another doesn't, you can compare the value lost or gained from the inclusion/exclusion of that variable. The good random forest implementations are going to do that for you, so all you need to do is know which method or variable to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View feature importance\n",
    "While we don't get regression coefficients like with ordinary least squares (OLS), we do get a score telling us how important each feature was in classifying. This is one of the most powerful parts of random forests, because we can clearly see that petal width was more important in classification than sepal width.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a list of the features and their importance scores\n",
    "list(zip(train[features], rf.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "1. Comment on the feature importances with respect to your previous observations on decision trees above.\n",
    "\n",
    "2. Extract and visualize 5 trees belonging to the random forest using the attribute `estimators_` of the trained random forest classifier. Compare them. *Note that you may code a loop on extracted trees.*\n",
    "\n",
    "3. Study the influence of parameters like `max_depth`, `min_samples_leaf` and `min_samples_split`. Try to optimize them and explain your approach and choices.\n",
    "\n",
    "4. How is estimated the prediction error of a random forest ?\n",
    "*Indication: have a look at parameter `oob_score`.*\n",
    "What are out-of-bag samples ?\n",
    "\n",
    "5. What should you do when classes are not balanced in the dataset ? (that is when there are much more examples of one class than another)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: a small example of regression using random forests\n",
    "Random forest is capable of learning without carefully crafted data transformations. Take the the $f(x) = \\sin(x)$ function for example.\n",
    "\n",
    "Create some fake data and add a little noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(-2.5, 2.5, 1000)\n",
    "y = np.sin(x) + np.random.normal(0, .1, 1000)\n",
    "\n",
    "plt.plot(x,y,'ko',markersize=1,label='data')\n",
    "plt.plot(np.arange(-2.5,2.5,0.1),np.sin(np.arange(-2.5,2.5,0.1)),'r-',label='ref')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try and build a basic linear model to predict y using x we end up with a straight line that sort of bisects the sin(x) function. Whereas if we use a random forest, it does a much better job of approximating the sin(x) curve and we get something that looks much more like the true function.\n",
    "\n",
    "Based on this example, we will illustrate how the random forest isn't bound by linear constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "1. Apply random forests on this dataset for regression and compare performances with ordinary least squares regression.\n",
    "*Note that ordinay least square regression is available thanks to:\n",
    "from sklearn.linear_model import LinearRegression*\n",
    "\n",
    "2. Comment on your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indications:\n",
    "You may use half of points for training and others to test predictions. Then you will have an idea of how far the random forest predictor fits the sinus curve.\n",
    "\n",
    "To this aim, you will need to use the model `RandomForestRegressor`. Be careful that when only 1 feature `x` is used as an input, you will need to reshape it by `x.reshape(-1,1)` when using methods `fit` and `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrf = RandomForestRegressor(n_estimators=30, max_depth=4)\n",
    "regrf.fit(x[0::2].reshape(-1, 1),y[0::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indication\n",
    "One clever way to compare models when using `scikit-learn`is to make a loop on models as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [DecisionTreeClassifier(max_depth=None),\n",
    "          RandomForestClassifier(n_estimators=n_estimators)]\n",
    "\n",
    "for model in models:\n",
    "    ...\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "### Decision trees\n",
    "http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "### Random forests\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "### Plot decision surface : using `plt.contourf`\n",
    "http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html#sphx-glr-auto-examples-tree-plot-iris-py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning trees: not available in scikit-learn.\n",
    "Since post-pruning of tree is not implemented in scikit-learn, you may think of coding your own pruning function. For instance, taking into account the numer of samples per leaf as proposed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pruning function (useful ?)\n",
    "def prune(decisiontree, min_samples_leaf = 1):\n",
    "    if decisiontree.min_samples_leaf >= min_samples_leaf:\n",
    "        raise Exception('Tree already more pruned')\n",
    "    else:\n",
    "        decisiontree.min_samples_leaf = min_samples_leaf\n",
    "        tree = decisiontree.tree_\n",
    "        for i in range(tree.node_count):\n",
    "            n_samples = tree.n_node_samples[i]\n",
    "            if n_samples <= min_samples_leaf:\n",
    "                tree.children_left[i]=-1\n",
    "                tree.children_right[i]=-1\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
