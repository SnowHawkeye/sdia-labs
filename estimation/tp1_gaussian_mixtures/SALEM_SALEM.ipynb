{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32403e8",
   "metadata": {},
   "source": [
    "# Compte-rendu du TP d'estimation statistique\n",
    "\n",
    "**Elève 1** : Hadrien SALEM\n",
    "\n",
    "**Elève 2** : Emilie SALEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf7ae0",
   "metadata": {},
   "source": [
    "## Import des fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d476b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import uniform\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093fcc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Densité de probabilité d'un mélange de gaussiennes en un point\n",
    "def gm_pdf(x, mu, sigma, p):\n",
    "    #Initialisation de la variable de sortie\n",
    "    resultat = 0.0\n",
    "    #Contrôle de la cohérence des paramètres d'entrée\n",
    "    #Le vecteur de moyenne doit avoir la même longueur que le vecteur p\n",
    "    if len(mu) != len(p):\n",
    "        print('Erreur de dimension sur la moyenne')\n",
    "    # Le vecteur des écart-types doit avoir la même longueur que le vecteur p\n",
    "    elif len(sigma) != len(p):\n",
    "            print('Erreur de dimension sur les écarts-types')\n",
    "    else:\n",
    "    # Calcul de la valeur de la densité\n",
    "        for i in range(0, len(p)):\n",
    "            resultat = resultat + p[i] * norm.pdf(x, mu[i], sigma[i])\n",
    "    return resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération de nombre aléatoire suivant un mélange de gaussiennes\n",
    "def gm_rnd(mu, sigma, p):\n",
    "    # Initialisation de la variable de sortie\n",
    "    resultat = 0.0\n",
    "    #Contrôle de la cohérence des paramètres d'entrée\n",
    "    #Le vecteur de moyenne doit avoir la même longueur que le vecteur p\n",
    "    if len(mu) != len(p):\n",
    "        print('Erreur de dimension sur la moyenne')\n",
    "    # Le vecteur des écart-types doit avoir la même longueur que le vecteur p\n",
    "    elif len(sigma) != len(p):\n",
    "            print('Erreur de dimension sur sur les écarts-types')\n",
    "    else:\n",
    "    #Génération de l'échantillon\n",
    "    # On échantillonne suivant une loi uniforme sur [0,1]\n",
    "        u = uniform.rvs(loc = 0.0, scale = 1.0, size = 1)\n",
    "    # % Chaque test suivant permet de définir un intervalle sur lequel la\n",
    "    # probabilité d'appartenance de la variable uniforme est égale à l'une des\n",
    "    # probabilités définie dans le vecteur p. Lorsque u appartient à l'un de\n",
    "    # ces intervalles, c'est équivalent à avoir générer une variable aléatoire\n",
    "    # suivant l'un des éléments de p. Par exemple, pour le premier test\n",
    "    # ci-dessous, la probabilité que u appartienne à l'intervalle [0,p[0][ est\n",
    "    # égale à p[0] puisque u suit une loi uniforme. Donc si u appartient à\n",
    "    # [0,p[0][ cela est équivalent à avoir tirer suivant l'événement de probabilité p[0].\n",
    "        if u < p[0]: # On test si on a généré un événement de probabilité p[0]\n",
    "            resultat = sigma[0] * norm.rvs(loc = 0, scale = 1, size = 1) + mu[0]\n",
    "            # Pour générer suivant une loi normale quelconque, il suffit de multiplier\n",
    "            # une variable normale centrée réduite (moyenne nulle et écart-type égal à 1)\n",
    "            # par l'écart-type désité et d'additionner la moyenne désirée au produit précédent.\n",
    "        for i in range(1, len(p)):\n",
    "            if (u > np.sum(p[0:i])) and (u <= np.sum(p[0:i+1])): # On test si on a généré\n",
    "                # un événement de probabilité p[i]\n",
    "                resultat = sigma[i] * norm.rvs(loc = 0.0, scale = 1.0, size = 1) + mu[i]\n",
    "                # Pour générer suivant une loi normale quelconque, il suffit de multiplier\n",
    "                # une variable normale centrée réduite (moyenne nulle et écart-type égal à 1)\n",
    "                # par l'écart-type désité et d'additionner la moyenne désirée au produit précédent.\n",
    "    return resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599686fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_algorithm(nbMaxIterations, mu_0, sigma_0, alpha_0, donnees) :\n",
    "\n",
    "    # Valeurs d'initialisation\n",
    "    mu_em = mu_0\n",
    "    sigma_em = sigma_0\n",
    "    alpha_em = alpha_0 # la somme doit faire 1\n",
    "\n",
    "    nbIteration = 1 #Initialisation de la variable d'arrêt\n",
    "    nbComposante = len(alpha_em) #Nombre de composantes du mélange\n",
    "    nbDonnees = len(donnees)  #Nombre de données\n",
    "    p = np.zeros(shape=(nbComposante, nbDonnees))\n",
    "    #Déclaration et initialisation de la matrice qui va contenir les probabilités\n",
    "    #p(k|x,theta_courant)\n",
    "\n",
    "    alpha_em_new = alpha_em\n",
    "    sigma_em_carre_new = sigma_em\n",
    "    mu_em_new = mu_em\n",
    "    donneesP = np.zeros(shape=(nbDonnees))\n",
    "\n",
    "    while nbIteration < nbMaxIterations:\n",
    "        for n in range(0, nbDonnees, 1):\n",
    "            for k in range(0, nbComposante, 1):\n",
    "                p[k, n] = alpha_em[k] * norm.pdf(x = donnees[n], loc = mu_em[k], scale = sigma_em[k])\n",
    "            p[:, n] = p[:, n] / np.sum(p[:, n])\n",
    "        for k in range(0, nbComposante, 1):\n",
    "            alpha_em_new[k] = np.sum(p[k,:]) / nbDonnees\n",
    "            for n in range(0, nbDonnees, 1):\n",
    "                donneesP[n] = donnees[n] * p[k, n]\n",
    "            mu_em_new[k]  = np.sum(donneesP) / np.sum(p[k, :])\n",
    "            for n in range(nbDonnees):\n",
    "                donneesP[n] = ((donnees[n] - mu_em_new[k]) ** 2) * p[k, n]\n",
    "            sigma_em_carre_new[k] = np.sum(donneesP) / np.sum(p[k, :])\n",
    "        mu_em = mu_em_new\n",
    "        sigma_em = np.sqrt(sigma_em_carre_new)\n",
    "        alpha_em = alpha_em_new\n",
    "        nbIteration = nbIteration + 1\n",
    "    \n",
    "    return np.array([mu_em, sigma_em, alpha_em])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa864bd3",
   "metadata": {},
   "source": [
    "## Partie 1 : test de l'algorithme EM sur des données synthétiques\n",
    "\n",
    "Après tests sur les données synthétiques, on remarque que:\n",
    "- Changer les valeurs initiales peut avoir une influence significative sur l'estimation. Prendre des valeurs \"proches\" des valeurs réelles permet d'obtenir une meilleure estimation. Il est donc préférable de déterminer un ordre de grandeur des paramètres recherchés avant même d'utiliser l'algorithme EM.\n",
    "- Pour un bon choix de paramètres initiaux, le nombre d'itérations semble avoir peu d'influence sur l'estimation. Il ne faut toutefois pas choisir une valeur trop faible, sans quoi l'algorithme n'a \"pas le temps\" de converger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f353a",
   "metadata": {},
   "source": [
    "## Partie 2 : Galaxies\n",
    "\n",
    "Nous allons essayer de proposer une estimation de la distribution des valeurs contenues dans `galaxies.txt` sous forme de mélange de gaussiennes.\n",
    "Il nous faut avant tout déterminer le nombre de gaussiennes du mélange, et pour chacune d'entre elles, une valeur d'initialisation pour :\n",
    "\n",
    "- sa moyenne $\\mu_0$\n",
    "- son écart-type $\\sigma_0$\n",
    "- sa \"probabilité\" $\\alpha_0$\n",
    "\n",
    "Pour simplifier la visualisation des données et éviter d'éventuels effets dûs aux grandes valeurs prises par les vitesses, nous les centrons et les réduisons.\n",
    "Dans les deux figures ci-dessous, nous affichons les données d'abord comme nuage de points, puis sous forme d'histogramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe les données\n",
    "x_galaxies = np.loadtxt(\"galaxies.txt\")\n",
    "x_galaxies = (x_galaxies - np.mean(x_galaxies)) / np.std(x_galaxies) # données centrées réduites\n",
    "nb_samples = len(x_galaxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des valeurs de chaque échantillon\n",
    "plt.plot(x_galaxies, 'g.')\n",
    "plt.title('Distribution des vitesses')\n",
    "plt.xlabel('Indice')\n",
    "plt.ylabel('Vitesse')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3861b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la distribution des valeurs sous forme d'histogramme\n",
    "def plot_galaxy_hist():\n",
    "    plt.hist(x_galaxies, bins = 30, density = True, edgecolor = \"black\")\n",
    "    plt.title('Distribution des vitesses')\n",
    "    plt.xlabel('Vitesse')\n",
    "    plt.ylabel(\"Nombre d'échantillons\")\n",
    "\n",
    "plot_galaxy_hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624698b",
   "metadata": {},
   "source": [
    "## Commentaire\n",
    "Dans un premier temps, nous considérerons les grandes tendances que nous pouvons voir sur ces deux courbes. Trois grands ensembles semblent se distinguer, nous choisirons donc un mélange de trois gaussiennes. Pour le choix des valeurs d'initialisation, nous calculons la moyenne et l'écart-type de trois ensembles, choisis pour l'instant \"à l'oeil\" en observant les courbes: \n",
    "- entre la première et la 8ème valeur\n",
    "- entre la 8ème et la 76ème valeur\n",
    "- et de la 76ème à la dernière valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d346aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix des valeurs d'initialisation et du nombre d'itérations pour 3 gaussiennes\n",
    "\n",
    "batches = np.split(x_galaxies, [8,76])\n",
    "mu_0 = np.array([np.mean(batch) for batch in batches])\n",
    "sigma_0 = np.array([np.std(batch) for batch in batches])\n",
    "alpha_0 = np.array([0.2, 0.7, 0.1])\n",
    "nb_iterations = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55653946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécution de l'algorithme EM pour 3 gaussiennes\n",
    "resultat = EM_algorithm(nb_iterations, mu_0, sigma_0, alpha_0, x_galaxies)\n",
    "mu_em, sigma_em, alpha_em = resultat\n",
    "\n",
    "def print_parameters(mu, sigma, alpha):\n",
    "    print('Les paramètres estimés sont : ')\n",
    "    print('Moyennes des composantes du mélange', mu)\n",
    "    print('Ecart type des composantes du mélange', sigma)\n",
    "    print('Probabilités des composantes du mélange', alpha)\n",
    "    \n",
    "print_parameters(mu_em, sigma_em, alpha_em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la distribution estimée, superposée à la distribution empirique des valeurs\n",
    "\n",
    "x = np.linspace(-3, 3, 10000)\n",
    "def plot_distribution_comparison(estimation):\n",
    "    plot_galaxy_hist()\n",
    "    plt.plot(x, estimation, 'r-', label = 'Estimée')\n",
    "    plt.legend(loc='upper left', shadow=True, fontsize='x-large')\n",
    "\n",
    "estimation_3_components = gm_pdf(x, mu_em, sigma_em, alpha_em)    \n",
    "plot_distribution_comparison(estimation_3_components)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire\n",
    "Au global, la distribution estimée suit les tendances de la distribution empirique. En revanche, pour les valeurs comprises entre -1 et 1, il semblerait que l'estimation pourrait être plus précise. En particulier, deux maxima locaux proches semblent se distinguer et ne sont pas différenciés par notre premier estimateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix des valeurs d'initialisation et du nombre d'itérations pour 4 gaussiennes\n",
    "\n",
    "batches = np.split(x_galaxies, [8,40,50])\n",
    "mu_0 = np.array([np.mean(batch) for batch in batches])\n",
    "sigma_0 = np.array([np.std(batch) for batch in batches])\n",
    "alpha_0 = np.array([0.2, 0.4, 0.3, 0.1])\n",
    "\n",
    "nb_iterations = 40\n",
    "resultat = EM_algorithm(nb_iterations, mu_0, sigma_0, alpha_0, x_galaxies)\n",
    "mu_em, sigma_em, alpha_em = resultat\n",
    "\n",
    "estimation_4_components = gm_pdf(x, mu_em, sigma_em, alpha_em)    \n",
    "plot_distribution_comparison(estimation_4_components)\n",
    "plt.show()\n",
    "\n",
    "print_parameters(mu_em, sigma_em, alpha_em)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire\n",
    "Avec un mélange de 4 gaussiennes, les deux maxima locaux du milieu sont mieux représentés, mais on perd en qualité pour la prédiction des vitesses moins probables (notamment les grandes vitesses, pour lesquelles on avait un pic distinct avec 3 gaussiennes).\n",
    "\n",
    "Nous pouvons essayer d'améliorer ce modèle en affinant le clustering des valeurs. Pour cela, nous faisons appel à la méthode des K-moyennes via `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_cluster(data, n_clusters):\n",
    "    kmeans = KMeans(n_clusters = n_clusters, random_state=0).fit(data.reshape(-1,1))\n",
    "    labeled_data = np.stack((kmeans.labels_, data), axis =1) # zip data with the label\n",
    "    labeled_data = labeled_data[labeled_data[:,0].argsort()] # order by label\n",
    "    return np.split(labeled_data[:,1], np.unique(labeled_data[:, 0], return_index = True)[1][1:]) # split values based on labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = kmeans_cluster(x_galaxies, 4)\n",
    "mu_0 = np.array([np.mean(batch) for batch in batches])\n",
    "sigma_0 = np.array([np.std(batch) for batch in batches])\n",
    "alpha_0 = np.array([0.2, 0.4, 0.3, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_iterations = 40\n",
    "resultat = EM_algorithm(nb_iterations, mu_0, sigma_0, alpha_0, x_galaxies)\n",
    "mu_em, sigma_em, alpha_em = resultat\n",
    "\n",
    "estimation_4_components = gm_pdf(x, mu_em, sigma_em, alpha_em)    \n",
    "plot_distribution_comparison(estimation_4_components)\n",
    "plt.show()\n",
    "\n",
    "print_parameters(mu_em, sigma_em, alpha_em)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire\n",
    "En appliquant cette méthode de clustering, on remarque que l'estimation est plus proche de la distribution empirique dans les cas \"peu probables\": on récupère le pic autour de 2.75, et on diminue l'erreur sur les ensembles de valeurs dont la densité est nulle. En revanche, les deux pics autour de 0 sont moins bien différenciés: on retrouve une distribution plus similaire à notre cas à 3 gaussiennes, bien que plus précise.\n",
    "\n",
    "Il nous faudrait donc une méthode pour choisir plus inteligemment le nombre de gaussiennes. Dans la suite, nous essayons d'utiliser le BIC (Bayesian Information Criterion) et AIC (Akaike Information Criterion) pour déterminer le nombre de gaussiennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20    \n",
    "bics = []\n",
    "aics = []\n",
    "for i in range(1, n):\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=i)\n",
    "    gmm.fit(x_galaxies.reshape(-1,1))\n",
    "    bics.append(gmm.bic(x_galaxies.reshape(-1,1)))\n",
    "    aics.append(gmm.aic(x_galaxies.reshape(-1,1)))\n",
    "    \n",
    "    \n",
    "plt.plot(np.arange(1,n),bics, label = 'BIC')\n",
    "plt.plot(np.arange(1,n),aics, label = 'AIC')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print(np.argmin(bics), np.argmin(aics))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire\n",
    "\n",
    "Les critères BIC et AIC permettent de favoriser la parcimonie, c'est-à-dire un équilibre entre la qualité de l'estimation et le nombre de paramètres du modèle. Avec le calcul ci-dessus, on détermine que le nombre de composantes du mélange devrait être 2 selon le critère BIC, et plus de 10 selon le critère AIC. Le BIC pénalise davantage le nombre de paramètres du modèle, en comparaison avec l'AIC, ce qui explique en partie cette différence.\n",
    "\n",
    "On notera qu'à chaque exécution de la cellule ci-dessus, les minimums de AIC et BIC sont susceptibles de changer (BIC prend les valeurs 2 et 4, AIC est très variable entre 12 et 18)\n",
    "\n",
    "Réalisons l'estimation dans ces deux cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_alpha_0(size):\n",
    "    return np.full((1,size), 1/size)[0]\n",
    "\n",
    "# Pour 2 gaussiennes\n",
    "batches = kmeans_cluster(x_galaxies, 2)\n",
    "mu_0 = np.array([np.mean(batch) for batch in batches])\n",
    "sigma_0 = np.array([np.std(batch) for batch in batches])\n",
    "alpha_0 = default_alpha_0(2)\n",
    "\n",
    "mu_em, sigma_em, alpha_em = EM_algorithm(nb_iterations, mu_0, sigma_0, alpha_0, x_galaxies)\n",
    "\n",
    "estimation = gm_pdf(x, mu_em, sigma_em, alpha_em)    \n",
    "plot_distribution_comparison(estimation)\n",
    "plt.show()\n",
    "\n",
    "# Pour 10 gaussiennes\n",
    "batches = kmeans_cluster(x_galaxies, 10)\n",
    "mu_0 = np.array([np.mean(batch) for batch in batches])\n",
    "sigma_0 = np.array([np.std(batch) for batch in batches])\n",
    "alpha_0 = default_alpha_0(10)\n",
    "\n",
    "mu_em, sigma_em, alpha_em = EM_algorithm(nb_iterations, mu_0, sigma_0, alpha_0, x_galaxies)\n",
    "\n",
    "estimation = gm_pdf(x, mu_em, sigma_em, alpha_em)    \n",
    "plot_distribution_comparison(estimation)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaire\n",
    "\n",
    "Nous notons premièrement que nous avons limité la deuxième estimation à 10 gaussiennes, car au-delà, les résultats ne semblaient pas s'afficher.\n",
    "\n",
    "On voit qu'en utilisant le BIC, le modèle estimé est simple. Seules les variations les plus distinctives sont représentées.\n",
    "A l'inverse, l'utilisation de 10 gaussiennes conduit à un modèle très complexe qui essaie de tenir compte de toutes les variations apparentes dans les données.\n",
    "\n",
    "On aura sans doute tendance à préférer un nombre intermédiaire de gaussiennes. Peut-être faudrait-il trouver un autre critère plus adapté au problème afin de déterminer le nombre de gaussiennes. Il faudrait également un critère pour déterminer l'efficacité du modèle sans connaissance du modèle initial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour 5 gaussiennes\n",
    "batches = kmeans_cluster(x_galaxies, 5)\n",
    "mu_0 = np.array([np.mean(batch) for batch in batches])\n",
    "sigma_0 = np.array([np.std(batch) for batch in batches])\n",
    "alpha_0 = default_alpha_0(5)\n",
    "\n",
    "mu_em, sigma_em, alpha_em = EM_algorithm(nb_iterations, mu_0, sigma_0, alpha_0, x_galaxies)\n",
    "\n",
    "estimation = gm_pdf(x, mu_em, sigma_em, alpha_em)    \n",
    "plot_distribution_comparison(estimation)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c7a02607861a5057e1d62a2006e259fdb8ddc0867b84f038715ad8928cc603b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('sdia-estimation': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
